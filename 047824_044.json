{"needle": "\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n", "haystack": "October 2015When I talk to a startup that's been operating for more than 8 or\n9 months, the first thing I want to know is almost always the same.\nAssuming their expenses remain constant and their revenue growth\nis what it has been over the last several months, do they make it to\nprofitability on the money they have left?  Or to put it more\ndramatically, by default do they live or die?The startling thing is how often the founders themselves don't know.\nHalf the founders I talk to don't know whether they're default alive\nor default dead.If you're among that number, Trevor Blackwell has made a handy\ncalculator you can use to find out.The reason I want to know first whether a startup is default alive\nor default dead is that the rest of the conversation depends on the\nanswer.  If the company is default alive, we can talk about ambitious\nnew things they could do.  If it's default dead, we probably need\nto talk about how to save it.  We know the current trajectory ends\nbadly.  How can they get off that trajectory?Why do so few founders know whether they're default alive or default\ndead?  Mainly, I think, because they're not used to asking that.\nIt's not a question that makes sense to ask early on, any more than\nit makes sense to ask a 3 year old how he plans to support\nhimself.  But as the company grows older, the question switches from\nmeaningless to critical.  That kind of switch often takes people\nby surprise.I propose the following solution: instead of starting to ask too\nlate whether you're default alive or default dead, start asking too\nearly.  It's hard to say precisely when the question switches\npolarity.  But it's probably not that dangerous to start worrying\ntoo early that you're default dead, whereas it's very dangerous to\nstart worrying too late.The reason is a phenomenon I wrote about earlier: the\nfatal pinch.\nThe fatal pinch is default dead + slow growth + not enough\ntime to fix it.  And the way founders end up in it is by not realizing\nthat's where they're headed.There is another reason founders don't ask themselves whether they're\ndefault alive or default dead: they assume it will be easy to raise\nmore money.  But that assumption is often false, and worse still, the\nmore you depend on it, the falser it becomes.Maybe it will help to separate facts from hopes. Instead of thinking\nof the future with vague optimism, explicitly separate the components.\nSay \"We're default dead, but we're counting on investors to save\nus.\" Maybe as you say that, it will set off the same alarms in your\nhead that it does in mine.  And if you set off the alarms sufficiently\nearly, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors\nsaving you.  As a rule their interest is a function of\ngrowth.  If you have steep revenue growth, say over 5x a year, you\ncan start to count on investors being interested even if you're not\nprofitable.\n[1]\nBut investors are so fickle that you can never\ndo more than start to count on them.  Sometimes something about your\nbusiness will spook investors even if your growth is great.  So no\nmatter how good your growth is, you can never safely treat fundraising\nas more than a plan A. You should always have a plan B as well: you\nshould know (as in write down) precisely what you'll need to do to\nsurvive if you can't raise more money, and precisely when you'll \nhave to switch to plan B if plan A isn't working.In any case, growing fast versus operating cheaply is far from the\nsharp dichotomy many founders assume it to be.  In practice there\nis surprisingly little connection between how much a startup spends\nand how fast it grows.  When a startup grows fast, it's usually\nbecause the product hits a nerve, in the sense of hitting some big\nneed straight on.  When a startup spends a lot, it's usually because\nthe product is expensive to develop or sell, or simply because\nthey're wasteful.If you're paying attention, you'll be asking at this point not just\nhow to avoid the fatal pinch, but how to avoid being default dead.\nThat one is easy: don't hire too fast.  Hiring too fast is by far\nthe biggest killer of startups that raise money.\n[2]Founders tell themselves they need to hire in order to grow.  But\nmost err on the side of overestimating this need rather than\nunderestimating it.  Why?  Partly because there's so much work to\ndo.  Naive founders think that if they can just hire enough\npeople, it will all get done.  Partly because successful startups have\nlots of employees, so it seems like that's what one does in order\nto be successful.  In fact the large staffs of successful startups\nare probably more the effect of growth than the cause.  And\npartly because when founders have slow growth they don't want to\nface what is usually the real reason: the product is not appealing\nenough.Plus founders who've just raised money are often encouraged to\noverhire by the VCs who funded them.  Kill-or-cure strategies are\noptimal for VCs because they're protected by the portfolio effect.\nVCs want to blow you up, in one sense of the phrase or the other.\nBut as a founder your incentives are different.  You want above all\nto survive.\n[3]Here's a common way startups die.  They make something moderately\nappealing and have decent initial growth. They raise their first\nround fairly easily, because the founders seem smart and the idea\nsounds plausible. But because the product is only moderately\nappealing, growth is ok but not great.  The founders convince\nthemselves that hiring a bunch of people is the way to boost growth.\nTheir investors agree.  But (because the product is only moderately\nappealing) the growth never comes.  Now they're rapidly running out\nof runway.  They hope further investment will save them. But because\nthey have high expenses and slow growth, they're now unappealing\nto investors. They're unable to raise more, and the company dies.What the company should have done is address the fundamental problem:\nthat the product is only moderately appealing.  Hiring people is\nrarely the way to fix that.  More often than not it makes it harder.\nAt this early stage, the product needs to evolve more than to be\n\"built out,\" and that's usually easier with fewer people.\n[4]Asking whether you're default alive or default dead may save you\nfrom this.  Maybe the alarm bells it sets off will counteract the\nforces that push you to overhire.  Instead you'll be compelled to\nseek growth in other ways. For example, by doing\nthings that don't scale, or by redesigning the product in the\nway only founders can.\nAnd for many if not most startups, these paths to growth will be\nthe ones that actually work.Airbnb waited 4 months after raising money at the end of Y\u00a0Combinator\nbefore they hired their first employee.  In the meantime the founders\nwere terribly overworked.  But they were overworked evolving Airbnb\ninto the astonishingly successful organism it is now.Notes[1]\nSteep usage growth will also interest investors.  Revenue\nwill ultimately be a constant multiple of usage, so x% usage growth\npredicts x% revenue growth.  But in practice investors discount\nmerely predicted revenue, so if you're measuring usage you need a\nhigher growth rate to impress investors.[2]\nStartups that don't raise money are saved from hiring too\nfast because they can't afford to. But that doesn't mean you should\navoid raising money in order to avoid this problem, any more than\nthat total abstinence is the only way to avoid becoming an alcoholic.[3]\nI would not be surprised if VCs' tendency to push founders\nto overhire is not even in their own interest.  They don't know how\nmany of the companies that get killed by overspending might have\ndone well if they'd survived.  My guess is a significant number.[4]\nAfter reading a draft, Sam Altman wrote:\"I think you should make the hiring point more strongly.  I think\nit's roughly correct to say that YC's most successful companies\nhave never been the fastest to hire, and one of the marks of a great\nfounder is being able to resist this urge.\"Paul Buchheit adds:\"A related problem that I see a lot is premature scaling\u2014founders\ntake a small business that isn't really working (bad unit economics,\ntypically) and then scale it up because they want impressive growth\nnumbers. This is similar to over-hiring in that it makes the business\nmuch harder to fix once it's big, plus they are bleeding cash really\nfast.\"\nThanks to Sam Altman, Paul Buchheit, Joe Gebbia, Jessica Livingston,\nand Geoff Ralston for reading drafts of this.November 2005In the next few years, venture capital funds will find themselves\nsqueezed from four directions.  They're already stuck with a seller's\nmarket, because of the huge amounts they raised at the end of the\nBubble and still haven't invested.  This by itself is not the end\nof the world.  In fact, it's just a more extreme version of the\nnorm\nin the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because\nit's getting so cheap to start a startup.  The four causes: open\nsource, which makes software free; Moore's law, which makes hardware\ngeometrically closer to free; the Web, which makes promotion free\nif you're good; and better languages, which make development a lot\ncheaper.When we started our startup in 1995, the first three were our biggest\nexpenses.  We had to pay $5000 for the Netscape Commerce Server,\nthe only software that then supported secure http connections.  We\npaid $3000 for a server with a 90 MHz processor and 32 meg of\nmemory.  And we paid a PR firm about $30,000 to promote our launch.Now you could get all three for nothing.  You can get the software\nfor free; people throw away computers more powerful than our first\nserver; and if you make something good you can generate ten times\nas much traffic by word of mouth online than our first PR firm got\nthrough the print media.And of course another big change for the average startup is that\nprogramming languages have improved-- or rather, the median language has.  At most startups ten years\nago, software development meant ten programmers writing code in\nC++.  Now the same work might be done by one or two using Python\nor Ruby.During the Bubble, a lot of people predicted that startups would\noutsource their development to India.  I think a better model for\nthe future is David Heinemeier Hansson, who outsourced his development\nto a more powerful language instead.  A lot of well-known applications\nare now, like BaseCamp, written by just one programmer.  And one\nguy is more than 10x cheaper than ten, because (a) he won't waste\nany time in meetings, and (b) since he's probably a founder, he can\npay himself nothing.Because starting a startup is so cheap, venture capitalists now\noften want to give startups more money than the startups want to\ntake.  VCs like to invest several million at a time.  But as one\nVC told me after a startup he funded would only take about half a\nmillion, \"I don't know what we're going to do.  Maybe we'll just\nhave to give some of it back.\" Meaning give some of the fund back\nto the institutional investors who supplied it, because it wasn't\ngoing to be possible to invest it all.Into this already bad situation comes the third problem: Sarbanes-Oxley.\nSarbanes-Oxley is a law, passed after the Bubble, that drastically\nincreases the regulatory burden on public companies. And in addition\nto the cost of compliance, which is at least two million dollars a\nyear, the law introduces frightening legal exposure for corporate\nofficers.  An experienced CFO I know said flatly: \"I would not\nwant to be CFO of a public company now.\"You might think that responsible corporate governance is an area\nwhere you can't go too far.  But you can go too far in any law, and\nthis remark convinced me that Sarbanes-Oxley must have.  This CFO\nis both the smartest and the most upstanding money guy I know.  If\nSarbanes-Oxley deters people like him from being CFOs of public  \ncompanies, that's proof enough that it's broken.Largely because of Sarbanes-Oxley, few startups go public now.  For\nall practical purposes, succeeding now equals getting bought.  Which\nmeans VCs are now in the business of finding promising little 2-3\nman startups and pumping them up into companies that cost $100\nmillion to acquire.   They didn't mean to be in this business; it's\njust what their business has evolved into.Hence the fourth problem: the acquirers have begun to realize they\ncan buy wholesale.  Why should they wait for VCs to make the startups\nthey want more expensive?  Most of what the VCs add, acquirers don't\nwant anyway.  The acquirers already have brand recognition and HR\ndepartments.  What they really want is the software and the developers,\nand that's what the startup is in the early phase: concentrated\nsoftware and developers.Google, typically, seems to have been the first to figure this out.\n\"Bring us your startups early,\" said Google's speaker at the Startup School.  They're quite\nexplicit about it: they like to acquire startups at just the point\nwhere they would do a Series A round.  (The Series A round is the\nfirst round of real VC funding; it usually happens in the first\nyear.) It is a brilliant strategy, and one that other big technology\ncompanies will no doubt try to duplicate.  Unless they want to have \nstill more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the\npeople there are rich, or expect to be when their options vest.\nOrdinary employees find it very hard to recommend an acquisition;\nit's just too annoying to see a bunch of twenty year olds get rich\nwhen you're still working for salary.  Even if it's the right thing   \nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\nThey need to do two things, one of which won't surprise them, and  \nanother that will seem an anathema.Let's start with the obvious one: lobby to get Sarbanes-Oxley  \nloosened.  This law was created to prevent future Enrons, not to\ndestroy the IPO market.  Since the IPO market was practically dead\nwhen it passed, few saw what bad effects it would have.  But now \nthat technology has recovered from the last bust, we can see clearly\nwhat a bottleneck Sarbanes-Oxley has become.Startups are fragile plants\u2014seedlings, in fact.  These seedlings\nare worth protecting, because they grow into the trees of the\neconomy.  Much of the economy's growth is their growth.  I think\nmost politicians realize that.  But they don't realize just how   \nfragile startups are, and how easily they can become collateral\ndamage of laws meant to fix some other problem.Still more dangerously, when you destroy startups, they make very\nlittle noise.  If you step on the toes of the coal industry, you'll\nhear about it.  But if you inadvertantly squash the startup industry,\nall that happens is that the founders of the next Google stay in \ngrad school instead of starting a company.My second suggestion will seem shocking to VCs: let founders cash  \nout partially in the Series A round.  At the moment, when VCs invest\nin a startup, all the stock they get is newly issued and all the \nmoney goes to the company.  They could buy some stock directly from\nthe founders as well.Most VCs have an almost religious rule against doing this.  They\ndon't want founders to get a penny till the company is sold or goes\npublic.  VCs are obsessed with control, and they worry that they'll\nhave less leverage over the founders if the founders have any money.This is a dumb plan.  In fact, letting the founders sell a little stock\nearly would generally be better for the company, because it would\ncause the founders' attitudes toward risk to be aligned with the\nVCs'.  As things currently work, their attitudes toward risk tend\nto be diametrically opposed: the founders, who have nothing, would\nprefer a 100% chance of $1 million to a 20% chance of $10 million,\nwhile the VCs can afford to be \"rational\" and prefer the latter.Whatever they say, the reason founders are selling their companies\nearly instead of doing Series A rounds is that they get paid up\nfront.  That first million is just worth so much more than the\nsubsequent ones.  If founders could sell a little stock early,\nthey'd be happy to take VC money and bet the rest on a bigger\noutcome.So why not let the founders have that first million, or at least\nhalf million?  The VCs would get same number of shares for the   \nmoney.  So what if some of the money would go to the  \nfounders instead of the company?Some VCs will say this is\nunthinkable\u2014that they want all their money to be put to work\ngrowing the company.  But the fact is, the huge size of current VC\ninvestments is dictated by the structure\nof VC funds, not the needs of startups.  Often as not these large  \ninvestments go to work destroying the company rather than growing\nit.The angel investors who funded our startup let the founders sell\nsome stock directly to them, and it was a good deal for everyone. \nThe angels made a huge return on that investment, so they're happy.\nAnd for us founders it blunted the terrifying all-or-nothingness\nof a startup, which in its raw form is more a distraction than a\nmotivator.If VCs are frightened at the idea of letting founders partially\ncash out, let me tell them something still more frightening: you\nare now competing directly with Google.\nThanks to Trevor Blackwell, Sarah Harlin, Jessica\nLivingston, and Robert Morris for reading drafts of this.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nNovember 2005Does \"Web 2.0\" mean anything?  Till recently I thought it didn't,\nbut the truth turns out to be more complicated.  Originally, yes,\nit was meaningless.  Now it seems to have acquired a meaning.  And\nyet those who dislike the term are probably right, because if it\nmeans what I think it does, we don't need it.I first heard the phrase \"Web 2.0\" in the name of the Web 2.0\nconference in 2004.  At the time it was supposed to mean using \"the\nweb as a platform,\" which I took to refer to web-based applications.\n[1]So I was surprised at a conference this summer when Tim O'Reilly\nled a session intended to figure out a definition of \"Web 2.0.\"\nDidn't it already mean using the web as a platform?  And if it\ndidn't already mean something, why did we need the phrase at all?OriginsTim says the phrase \"Web 2.0\" first\narose in \"a brainstorming session between\nO'Reilly and Medialive International.\" What is Medialive International?\n\"Producers of technology tradeshows and conferences,\" according to\ntheir site.  So presumably that's what this brainstorming session\nwas about.  O'Reilly wanted to organize a conference about the web,\nand they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a\nnew version of the web.  They just wanted to make the point\nthat the web mattered again.  It was a kind of semantic deficit\nspending: they knew new things were coming, and the \"2.0\" referred\nto whatever those might turn out to be.And they were right.  New things were coming.  But the new version\nnumber led to some awkwardness in the short term.  In the process\nof developing the pitch for the first conference, someone must have\ndecided they'd better take a stab at explaining what that \"2.0\"\nreferred to.  Whatever it meant, \"the web as a platform\" was at\nleast not too constricting.The story about \"Web 2.0\" meaning the web as a platform didn't live\nmuch past the first conference.  By the second conference, what\n\"Web 2.0\" seemed to mean was something about democracy.  At least,\nit did when people wrote about it online.  The conference itself\ndidn't seem very grassroots.  It cost $2800, so the only people who\ncould afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article\nabout the conference in Wired News spoke of \"throngs of\ngeeks.\"  When a friend of mine asked Ryan about this, it was news\nto him.  He said he'd originally written something like \"throngs\nof VCs and biz dev guys\" but had later shortened it just to \"throngs,\"\nand that this must have in turn been expanded by the editors into\n\"throngs of geeks.\"  After all, a Web 2.0 conference would presumably\nbe full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a   \nsuit, a sight so alien I couldn't parse it at first.  I saw\nhim walk by and said to one of the O'Reilly people \"that guy looks\njust like Tim.\"\"Oh, that's Tim.  He bought a suit.\"\nI ran after him, and sure enough, it was.  He explained that he'd\njust bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows\nduring the Bubble, full of prowling VCs looking for the next hot\nstartup.  There was that same odd atmosphere created by a large  \nnumber of people determined not to miss out.  Miss out on what?\nThey didn't know.  Whatever was going to happen\u2014whatever Web 2.0\nturned out to be.I wouldn't quite call it \"Bubble 2.0\" just because VCs are eager\nto invest again.  The Internet is a genuinely big deal.  The bust\nwas as much an overreaction as\nthe boom.  It's to be expected that once we started to pull out of\nthe bust, there would be a lot of growth in this area, just as there\nwas in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO\nmarket is gone.  Venture investors\nare driven by exit strategies.  The reason they were funding all  \nthose laughable startups during the late 90s was that they hoped\nto sell them to gullible retail investors; they hoped to be laughing\nall the way to the bank.  Now that route is closed.  Now the default\nexit strategy is to get bought, and acquirers are less prone to\nirrational exuberance than IPO investors.  The closest you'll get \nto Bubble valuations is Rupert Murdoch paying $580 million for   \nMyspace.  That's only off by a factor of 10 or so.1. AjaxDoes \"Web 2.0\" mean anything more than the name of a conference\nyet?  I don't like to admit it, but it's starting to.  When people\nsay \"Web 2.0\" now, I have some idea what they mean.  And the fact\nthat I both despise the phrase and understand it is the surest proof\nthat it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still\nonly just bear to use without scare quotes.  Basically, what \"Ajax\"\nmeans is \"Javascript now works.\"  And that in turn means that\nweb-based applications can now be made to work much more like desktop\nones.As you read this, a whole new generation\nof software is being written to take advantage of Ajax.  There\nhasn't been such a wave of new applications since microcomputers\nfirst appeared.  Even Microsoft sees it, but it's too late for them\nto do anything more than leak \"internal\"  \ndocuments designed to give the impression they're on top of this\nnew trend.In fact the new generation of software is being written way too\nfast for Microsoft even to channel it, let alone write their own\nin house.  Their only hope now is to buy all the best Ajax startups\nbefore Google does.  And even that's going to be hard, because\nGoogle has as big a head start in buying microstartups as it did\nin search a few years ago.  After all, Google Maps, the canonical\nAjax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference\nturned out to be partially right: web-based applications are a big\ncomponent of Web 2.0.  But I'm convinced they got this right by \naccident.  The Ajax boom didn't start till early 2005, when Google\nMaps appeared and the term \"Ajax\" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several\nexamples to prove that amateurs can   \nsurpass professionals, when they have the right kind of system to \nchannel their efforts.  Wikipedia\nmay be the most famous.  Experts have given Wikipedia middling\nreviews, but they miss the critical point: it's good enough.  And   \nit's free, which means people actually read it.  On the web, articles\nyou have to pay for might as well not exist.  Even if you were    \nwilling to pay to read them yourself, you can't link to them.    \nThey're not part of the conversation.Another place democracy seems to win is in deciding what counts as\nnews.  I never look at any news site now except Reddit.\n[2]\n I know if something major\nhappens, or someone writes a particularly interesting article, it   \nwill show up there.  Why bother checking the front page of any\nspecific paper or magazine?  Reddit's like an RSS feed for the whole\nweb, with a filter for quality.  Similar sites include Digg, a technology news site that's\nrapidly approaching Slashdot in popularity, and del.icio.us, the collaborative\nbookmarking network that set off the \"tagging\" movement.  And whereas\nWikipedia's main appeal is that it's good enough and free, these\nsites suggest that voters do a significantly better job than human\neditors.The most dramatic example of Web 2.0 democracy is not in the selection\nof ideas, but their production.  \nI've noticed for a while that the stuff I read on individual people's\nsites is as good as or better than the stuff I read in newspapers\nand magazines.  And now I have independent evidence: the top links\non Reddit are generally links to individual people's sites rather  \nthan to magazine articles or news stories.My experience of writing\nfor magazines suggests an explanation.  Editors.  They control the\ntopics you can write about, and they can generally rewrite whatever\nyou produce.  The result is to damp extremes.  Editing yields 95th\npercentile writing\u201495% of articles are improved by it, but 5% are\ndragged down.  5% of the time you get \"throngs of geeks.\"On the web, people can publish whatever they want.  Nearly all of\nit falls short of the editor-damped writing in print publications.\nBut the pool of writers is very, very large.  If it's large enough,\nthe lack of damping means the best writing online should surpass  \nthe best in print.\n[3]  \nAnd now that the web has evolved mechanisms\nfor selecting good stuff, the web wins net.  Selection beats damping,\nfor the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the  \nstartups of the Bubble what bloggers are to the print media.  During\nthe Bubble, a startup meant a company headed by an MBA that was   \nblowing through several million dollars of VC money to \"get big\nfast\" in the most literal sense.  Now it means a smaller, younger, more technical group that just      \ndecided to make something great.  They'll decide later if they want  \nto raise VC-scale funding, and if they take it, they'll take it on\ntheir terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements\nof \"Web 2.0.\"  I also see a third: not to maltreat users.  During\nthe Bubble a lot of popular sites were quite high-handed with users.\nAnd not just in obvious ways, like making them register, or subjecting\nthem to annoying ads.  The very design of the average site in the   \nlate 90s was an abuse.  Many of the most popular sites were loaded\nwith obtrusive branding that made them slow to load and sent the\nuser the message: this is our site, not yours.  (There's a physical\nanalog in the Intel and Microsoft stickers that come on some\nlaptops.)I think the root of the problem was that sites felt they were giving\nsomething away for free, and till recently a company giving anything\naway for free could be pretty high-handed about it.  Sometimes it\nreached the point of economic sadism: site owners assumed that the\nmore pain they caused the user, the more benefit it must be to them.  \nThe most dramatic remnant of this model may be at salon.com, where   \nyou can read the beginning of a story, but to get the rest you have\nsit through a movie.At Y Combinator we advise all the startups we fund never to lord\nit over users.  Never make users register, unless you need to in\norder to store something for them.  If you do make users register,   \nnever make them wait for a confirmation link in an email; in fact,\ndon't even ask for their email address unless you need it for some\nreason.  Don't ask them any unnecessary questions.  Never send them\nemail unless they explicitly ask for it.  Never frame pages you\nlink to, or open them in new windows.  If you have a free version \nand a pay version, don't make the free version too restricted.  And\nif you find yourself asking \"should we allow users to do x?\" just \nanswer \"yes\" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups\nnever to let anyone fly under them, meaning never to let any other\ncompany offer a cheaper, easier solution.  Another way to fly low \nis to give users more power.  Let users do what they want.  If you \ndon't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual\nsongs instead of having to buy whole albums.  The recording industry\nhated the idea and resisted it as long as possible.  But it was\nobvious what users wanted, so Apple flew under the labels.\n[4]\nThough really it might be better to describe iTunes as Web 1.5.     \nWeb 2.0 applied to music would probably mean individual bands giving\naway DRMless songs for free.The ultimate way to be nice to users is to give them something for\nfree that competitors charge for.  During the 90s a lot of people   \nprobably thought we'd have some working system for micropayments     \nby now.  In fact things have gone in the other direction.  The most   \nsuccessful sites are the ones that figure out new ways to give stuff\naway for free.  Craigslist has largely destroyed the classified ad\nsites of the 90s, and OkCupid looks likely to do the same to the\nprevious generation of dating sites.Serving web pages is very, very cheap.  If you can make even a   \nfraction of a cent per page view, you can make a profit.  And\ntechnology for targeting ads continues to improve.  I wouldn't be\nsurprised if ten years from now eBay had been supplanted by an      \nad-supported freeBay (or, more likely, gBay).Odd as it might sound, we tell startups that they should try to\nmake as little money as possible.  If you can figure out a way to\nturn a billion dollar industry into a fifty million dollar industry,\nso much the better, if all fifty million go to you.  Though indeed,\nmaking things cheaper often turns out to generate more money in the\nend, just as automating things often turns out to generate more\njobs.The ultimate target is Microsoft.  What a bang that balloon is going\nto make when someone pops it by offering a free web-based alternative \nto MS Office.\n[5]\nWho will?  Google?  They seem to be taking their\ntime.  I suspect the pin will be wielded by a couple of 20 year old\nhackers who are too naive to be intimidated by the idea.  (How hard\ncan it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in  \ncommon?  I didn't realize they had anything in common till recently,\nwhich is one of the reasons I disliked the term \"Web 2.0\" so much.\nIt seemed that it was being used as a label for whatever happened\nto be new\u2014that it didn't predict anything.But there is a common thread.  Web 2.0 means using the web the way\nit's meant to be used.  The \"trends\" we're seeing now are simply\nthe inherent nature of the web emerging from under the broken models\nthat got imposed on it during the Bubble.I realized this when I read an  interview with\nJoe Kraus, the co-founder of Excite.\n[6]\n\n  Excite really never got the business model right at all.  We fell \n  into the classic problem of how when a new medium comes out it\n  adopts the practices, the content, the business models of the old\n  medium\u2014which fails, and then the more appropriate models get\n  figured out.\n\nIt may have seemed as if not much was happening during the years\nafter the Bubble burst.  But in retrospect, something was happening:\nthe web was finding its natural angle of repose.  The democracy \ncomponent, for example\u2014that's not an innovation, in the sense of\nsomething someone made happen.  That's what the web naturally tends\nto produce.Ditto for the idea of delivering desktop-like applications over the\nweb.  That idea is almost as old as the web.  But the first time    \naround it was co-opted by Sun, and we got Java applets.  Java has\nsince been remade into a generic replacement for C++, but in 1996\nthe story about Java was that it represented a new model of software.\nInstead of desktop applications, you'd run Java \"applets\" delivered\nfrom a server.This plan collapsed under its own weight. Microsoft helped kill it,\nbut it would have died anyway.  There was no uptake among hackers.\nWhen you find PR firms promoting\nsomething as the next development platform, you can be sure it's\nnot.  If it were, you wouldn't need PR firms to tell you, because   \nhackers would already be writing stuff on top of it, the way sites    \nlike Busmonster used Google Maps as a\nplatform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of  \nhackers have spontaneously started building things on top\nof it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common.\nHere's a clue.  Suppose you approached investors with the following\nidea for a Web 2.0 startup:\n\n  Sites like del.icio.us and flickr allow users to \"tag\" content\n  with descriptive tokens.  But there is also huge source of\n  implicit tags that they ignore: the text within web links.\n  Moreover, these links represent a social network connecting the   \n  individuals and organizations who created the pages, and by using\n  graph theory we can compute from this network an estimate of the\n  reputation of each member.  We plan to mine the web for these \n  implicit tags, and use them together with the reputation hierarchy\n  they embody to enhance web searches.\n\nHow long do you think it would take them on average to realize that\nit was a description of Google?Google was a pioneer in all three components of Web 2.0: their core\nbusiness sounds crushingly hip when described in Web 2.0 terms, \n\"Don't maltreat users\" is a subset of \"Don't be evil,\" and of course\nGoogle set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google\ndoes.  That's their secret.    They're sailing with the wind, instead of sitting  \nbecalmed praying for a business model, like the print media, or   \ntrying to tack upwind by suing their customers, like Microsoft and \nthe record labels.\n[7]Google doesn't try to force things to happen their way.  They try   \nto figure out what's going to happen, and arrange to be standing \nthere when it does.  That's the way to approach technology\u2014and \nas business includes an ever larger technological component, the\nright way to do business.The fact that Google is a \"Web 2.0\" company shows that, while\nmeaningful, the term is also rather bogus.  It's like the word\n\"allopathic.\"  It just means doing things right, and it's a bad   \nsign when you have a special word for that.\nNotes[1]\nFrom the conference\nsite, June 2004: \"While the first wave of the Web was closely  \ntied to the browser, the second wave extends applications across    \nthe web and enables a new generation of services and business\nopportunities.\"  To the extent this means anything, it seems to be\nabout \nweb-based applications.[2]\nDisclosure: Reddit was funded by \nY Combinator.  But although\nI started using it out of loyalty to the home team, I've become a\ngenuine addict.  While we're at it, I'm also an investor in\n!MSFT, having sold all my shares earlier this year.[3]\nI'm not against editing. I spend more time editing than\nwriting, and I have a group of picky friends who proofread almost\neverything I write.  What I dislike is editing done after the fact  \nby someone else.[4]\nObvious is an understatement.  Users had been climbing in through  \nthe window for years before Apple finally moved the door.[5]\nHint: the way to create a web-based alternative to Office may\nnot be to write every component yourself, but to establish a protocol\nfor web-based apps to share a virtual home directory spread across\nmultiple servers.  Or it may be to write it all yourself.[6]\nIn Jessica Livingston's\nFounders at\nWork.[7]\nMicrosoft didn't sue their customers directly, but they seem \nto have done all they could to help SCO sue them.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, Peter\nNorvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the\nguys at O'Reilly and Adaptive Path for answering my questions.January 2003(This article is derived from a keynote talk at the fall 2002 meeting\nof NEPLS.)Visitors to this country are often surprised to find that\nAmericans like to begin a conversation by asking \"what do you do?\"\nI've never liked this question.  I've rarely had a\nneat answer to it.  But I think I have finally solved the problem.\nNow, when someone asks me what I do, I look them straight\nin the eye and say \"I'm designing a \nnew dialect of Lisp.\"   \nI recommend this answer to anyone who doesn't like being asked what\nthey do.  The conversation will turn immediately to other topics.I don't consider myself to be doing research on programming languages.\nI'm just designing one, in the same way that someone might design\na building or a chair or a new typeface.\nI'm not trying to discover anything new.  I just want\nto make a language that will be good to program in.  In some ways,\nthis assumption makes life a lot easier.The difference between design and research seems to be a question\nof new versus good.  Design doesn't have to be new, but it has to  \nbe good.  Research doesn't have to be good, but it has to be new.\nI think these two paths converge at the top: the best design\nsurpasses its predecessors by using new ideas, and the best research\nsolves problems that are not only new, but actually worth solving.\nSo ultimately we're aiming for the same destination, just approaching\nit from different directions.What I'm going to talk about today is what your target looks like\nfrom the back.  What do you do differently when you treat\nprogramming languages as a design problem instead of a research topic?The biggest difference is that you focus more on the user.\nDesign begins by asking, who is this\nfor and what do they need from it?  A good architect,\nfor example, does not begin by creating a design that he then\nimposes on the users, but by studying the intended users and figuring\nout what they need.Notice I said \"what they need,\" not \"what they want.\"  I don't mean\nto give the impression that working as a designer means working as \na sort of short-order cook, making whatever the client tells you\nto.  This varies from field to field in the arts, but\nI don't think there is any field in which the best work is done by\nthe people who just make exactly what the customers tell them to.The customer is always right in\nthe sense that the measure of good design is how well it works\nfor the user.  If you make a novel that bores everyone, or a chair\nthat's horribly uncomfortable to sit in, then you've done a bad\njob, period.  It's no defense to say that the novel or the chair  \nis designed according to the most advanced theoretical principles.And yet, making what works for the user doesn't mean simply making\nwhat the user tells you to.  Users don't know what all the choices\nare, and are often mistaken about what they really want.The answer to the paradox, I think, is that you have to design\nfor the user, but you have to design what the user needs, not simply  \nwhat he says he wants.\nIt's much like being a doctor.  You can't just treat a patient's\nsymptoms.  When a patient tells you his symptoms, you have to figure\nout what's actually wrong with him, and treat that.This focus on the user is a kind of axiom from which most of the\npractice of good design can be derived, and around which most design\nissues center.If good design must do what the user needs, who is the user?  When\nI say that design must be for users, I don't mean to imply that good \ndesign aims at some kind of  \nlowest common denominator.  You can pick any group of users you\nwant.  If you're designing a tool, for example, you can design it\nfor anyone from beginners to experts, and what's good design\nfor one group might be bad for another.  The point\nis, you have to pick some group of users.  I don't think you can\neven talk about good or bad design except with\nreference to some intended user.You're most likely to get good design if the intended users include\nthe designer himself.  When you design something\nfor a group that doesn't include you, it tends to be for people\nyou consider to be less sophisticated than you, not more sophisticated.That's a problem, because looking down on the user, however benevolently,\nseems inevitably to corrupt the designer.\nI suspect that very few housing\nprojects in the US were designed by architects who expected to live\nin them.   You can see the same thing\nin programming languages.  C, Lisp, and Smalltalk were created for\ntheir own designers to use.  Cobol, Ada, and Java, were created   \nfor other people to use.If you think you're designing something for idiots, the odds are\nthat you're not designing something good, even for idiots.\nEven if you're designing something for the most sophisticated\nusers, though, you're still designing for humans.  It's different \nin research.  In math you\ndon't choose abstractions because they're\neasy for humans to understand; you choose whichever make the\nproof shorter.  I think this is true for the sciences generally.\nScientific ideas are not meant to be ergonomic.Over in the arts, things are very different.  Design is\nall about people.  The human body is a strange\nthing, but when you're designing a chair,\nthat's what you're designing for, and there's no way around it.\nAll the arts have to pander to the interests and limitations\nof humans.   In painting, for example, all other things being\nequal a painting with people in it will be more interesting than\none without.  It is not merely an accident of history that\nthe great paintings of the Renaissance are all full of people.\nIf they hadn't been, painting as a medium wouldn't have the prestige\nthat it does.Like it or not, programming languages are also for people,\nand I suspect the human brain is just as lumpy and idiosyncratic\nas the human body.  Some ideas are easy for people to grasp\nand some aren't.  For example, we seem to have a very limited\ncapacity for dealing with detail.  It's this fact that makes\nprograming languages a good idea in the first place; if we\ncould handle the detail, we could just program in machine\nlanguage.Remember, too, that languages are not\nprimarily a form for finished programs, but something that\nprograms have to be developed in.  Anyone in the arts could\ntell you that you might want different mediums for the\ntwo situations.  Marble, for example, is a nice, durable\nmedium for finished ideas, but a hopelessly inflexible one\nfor developing new ideas.A program, like a proof,\nis a pruned version of a tree that in the past has had\nfalse starts branching off all over it.  So the test of\na language is not simply how clean the finished program looks\nin it, but how clean the path to the finished program was.\nA design choice that gives you elegant finished programs\nmay not give you an elegant design process.  For example, \nI've written a few macro-defining macros full of nested\nbackquotes that look now like little gems, but writing them\ntook hours of the ugliest trial and error, and frankly, I'm still\nnot entirely sure they're correct.We often act as if the test of a language were how good\nfinished programs look in it.\nIt seems so convincing when you see the same program\nwritten in two languages, and one version is much shorter.\nWhen you approach the problem from the direction of the\narts, you're less likely to depend on this sort of\ntest.  You don't want to end up with a programming\nlanguage like marble.For example, it is a huge win in developing software to\nhave an interactive toplevel, what in Lisp is called a\nread-eval-print loop.  And when you have one this has\nreal effects on the design of the language.  It would not\nwork well for a language where you have to declare\nvariables before using them, for example.  When you're\njust typing expressions into the toplevel, you want to be \nable to set x to some value and then start doing things\nto x.  You don't want to have to declare the type of x\nfirst.  You may dispute either of the premises, but if\na language has to have a toplevel to be convenient, and\nmandatory type declarations are incompatible with a\ntoplevel, then no language that makes type declarations  \nmandatory could be convenient to program in.In practice, to get good design you have to get close, and stay\nclose, to your users.  You have to calibrate your ideas on actual\nusers constantly, especially in the beginning.  One of the reasons\nJane Austen's novels are so good is that she read them out loud to\nher family.  That's why she never sinks into self-indulgently arty\ndescriptions of landscapes,\nor pretentious philosophizing.  (The philosophy's there, but it's\nwoven into the story instead of being pasted onto it like a label.)\nIf you open an average \"literary\" novel and imagine reading it out loud\nto your friends as something you'd written, you'll feel all too\nkeenly what an imposition that kind of thing is upon the reader.In the software world, this idea is known as Worse is Better.\nActually, there are several ideas mixed together in the concept of\nWorse is Better, which is why people are still arguing about\nwhether worse\nis actually better or not.  But one of the main ideas in that\nmix is that if you're building something new, you should get a\nprototype in front of users as soon as possible.The alternative approach might be called the Hail Mary strategy.\nInstead of getting a prototype out quickly and gradually refining\nit, you try to create the complete, finished, product in one long\ntouchdown pass.  As far as I know, this is a\nrecipe for disaster.  Countless startups destroyed themselves this\nway during the Internet bubble.  I've never heard of a case\nwhere it worked.What people outside the software world may not realize is that\nWorse is Better is found throughout the arts.\nIn drawing, for example, the idea was discovered during the\nRenaissance.  Now almost every drawing teacher will tell you that\nthe right way to get an accurate drawing is not to\nwork your way slowly around the contour of an object, because errors will\naccumulate and you'll find at the end that the lines don't meet.\nInstead you should draw a few quick lines in roughly the right place,\nand then gradually refine this initial sketch.In most fields, prototypes\nhave traditionally been made out of different materials.\nTypefaces to be cut in metal were initially designed  \nwith a brush on paper.  Statues to be cast in bronze   \nwere modelled in wax.  Patterns to be embroidered on tapestries\nwere drawn on paper with ink wash.  Buildings to be\nconstructed from stone were tested on a smaller scale in wood.What made oil paint so exciting, when it\nfirst became popular in the fifteenth century, was that you\ncould actually make the finished work from the prototype.\nYou could make a preliminary drawing if you wanted to, but you\nweren't held to it; you could work out all the details, and\neven make major changes, as you finished the painting.You can do this in software too.  A prototype doesn't have to\nbe just a model; you can refine it into the finished product.\nI think you should always do this when you can.  It lets you\ntake advantage of new insights you have along the way.  But\nperhaps even more important, it's good for morale.Morale is key in design.  I'm surprised people\ndon't talk more about it.  One of my first\ndrawing teachers told me: if you're bored when you're\ndrawing something, the drawing will look boring.\nFor example, suppose you have to draw a building, and you\ndecide to draw each brick individually.  You can do this\nif you want, but if you get bored halfway through and start\nmaking the bricks mechanically instead of observing each one,   \nthe drawing will look worse than if you had merely suggested\nthe bricks.Building something by gradually refining a prototype is good\nfor morale because it keeps you engaged.  In software, my  \nrule is: always have working code.  If you're writing\nsomething that you'll be able to test in an hour, then you\nhave the prospect of an immediate reward to motivate you.\nThe same is true in the arts, and particularly in oil painting.\nMost painters start with a blurry sketch and gradually\nrefine it.\nIf you work this way, then in principle\nyou never have to end the day with something that actually\nlooks unfinished.  Indeed, there is even a saying among\npainters: \"A painting is never finished, you just stop\nworking on it.\"  This idea will be familiar to anyone who\nhas worked on software.Morale is another reason that it's hard to design something\nfor an unsophisticated user.   It's hard to stay interested in\nsomething you don't like yourself.  To make something  \ngood, you have to be thinking, \"wow, this is really great,\"\nnot \"what a piece of shit; those fools will love it.\"Design means making things for humans.  But it's not just the\nuser who's human.  The designer is human too.Notice all this time I've been talking about \"the designer.\"\nDesign usually has to be under the control of a single person to\nbe any good.   And yet it seems to be possible for several people\nto collaborate on a research project.  This seems to\nme one of the most interesting differences between research and\ndesign.There have been famous instances of collaboration in the arts,\nbut most of them seem to have been cases of molecular bonding rather\nthan nuclear fusion.  In an opera it's common for one person to\nwrite the libretto and another to write the music.   And during the Renaissance, \njourneymen from northern\nEurope were often employed to do the landscapes in the\nbackgrounds of Italian paintings.  But these aren't true collaborations.\nThey're more like examples of Robert Frost's\n\"good fences make good neighbors.\"  You can stick instances\nof good design together, but within each individual project,\none person has to be in control.I'm not saying that good design requires that one person think\nof everything.  There's nothing more valuable than the advice\nof someone whose judgement you trust.  But after the talking is\ndone, the decision about what to do has to rest with one person.Why is it that research can be done by collaborators and  \ndesign can't?  This is an interesting question.  I don't \nknow the answer.  Perhaps,\nif design and research converge, the best research is also\ngood design, and in fact can't be done by collaborators.\nA lot of the most famous scientists seem to have worked alone.\nBut I don't know enough to say whether there\nis a pattern here.  It could be simply that many famous scientists\nworked when collaboration was less common.Whatever the story is in the sciences, true collaboration\nseems to be vanishingly rare in the arts.  Design by committee is a\nsynonym for bad design.  Why is that so?  Is there some way to\nbeat this limitation?I'm inclined to think there isn't-- that good design requires\na dictator.  One reason is that good design has to   \nbe all of a piece.  Design is not just for humans, but\nfor individual humans.  If a design represents an idea that  \nfits in one person's head, then the idea will fit in the user's\nhead too.Related:April 2005\"Suits make a corporate comeback,\" says the New\nYork Times.  Why does this sound familiar?  Maybe because\nthe suit was also back in February,\n\nSeptember\n2004, June\n2004, March\n2004, September\n2003, \n\nNovember\n2002, \nApril 2002,\nand February\n2002.\n\nWhy do the media keep running stories saying suits are back?  Because\nPR firms tell \nthem to.  One of the most surprising things I discovered\nduring my brief business career was the existence of the PR industry,\nlurking like a huge, quiet submarine beneath the news.  Of the\nstories you read in traditional media that aren't about politics,\ncrimes, or disasters, more than half probably come from PR firms.I know because I spent years hunting such \"press hits.\"  Our startup spent\nits entire marketing budget on PR: at a time when we were assembling\nour own computers to save money, we were paying a PR firm $16,000\na month.  And they were worth it.  PR is the news equivalent of\nsearch engine optimization; instead of buying ads, which readers\nignore, you get yourself inserted directly into the stories.  [1]Our PR firm\nwas one of the best in the business.  In 18 months, they got press\nhits in over 60 different publications.  \nAnd we weren't the only ones they did great things for.  \nIn 1997 I got a call from another\nstartup founder considering hiring them to promote his company.  I\ntold him they were PR gods, worth every penny of their outrageous   \nfees.  But I remember thinking his company's name was odd.\nWhy call an auction site \"eBay\"?\nSymbiosisPR is not dishonest.  Not quite.  In fact, the reason the best PR\nfirms are so effective is precisely that they aren't dishonest.\nThey give reporters genuinely valuable information.  A good PR firm\nwon't bug reporters just because the client tells them to; they've\nworked hard to build their credibility with reporters, and they\ndon't want to destroy it by feeding them mere propaganda.If anyone is dishonest, it's the reporters.  The main reason PR  \nfirms exist is that reporters are lazy.  Or, to put it more nicely,\noverworked.  Really they ought to be out there digging up stories\nfor themselves.  But it's so tempting to sit in their offices and\nlet PR firms bring the stories to them.  After all, they know good\nPR firms won't lie to them.A good flatterer doesn't lie, but tells his victim selective truths\n(what a nice color your eyes are). Good PR firms use the same\nstrategy: they give reporters stories that are true, but whose truth\nfavors their clients.For example, our PR firm often pitched stories about how the Web  \nlet small merchants compete with big ones.  This was perfectly true.\nBut the reason reporters ended up writing stories about this\nparticular truth, rather than some other one, was that small merchants\nwere our target market, and we were paying the piper.Different publications vary greatly in their reliance on PR firms.\nAt the bottom of the heap are the trade press, who make most of\ntheir money from advertising and would give the magazines away for\nfree if advertisers would let them.  [2] The average\ntrade publication is a  bunch of ads, glued together by just enough\narticles to make it look like a magazine.  They're so desperate for\n\"content\" that some will print your press releases almost verbatim,\nif you take the trouble to write them to read like articles.At the other extreme are publications like the New York Times\nand the Wall Street Journal.  Their reporters do go out and\nfind their own stories, at least some of the time.  They'll listen \nto PR firms, but briefly and skeptically.  We managed to get press   \nhits in almost every publication we wanted, but we never managed \nto crack the print edition of the Times.  [3]The weak point of the top reporters is not laziness, but vanity.\nYou don't pitch stories to them.  You have to approach them as if\nyou were a specimen under their all-seeing microscope, and make it\nseem as if the story you want them to run is something they thought \nof themselves.Our greatest PR coup was a two-part one.  We estimated, based on\nsome fairly informal math, that there were about 5000 stores on the\nWeb.  We got one paper to print this number, which seemed neutral   \nenough.  But once this \"fact\" was out there in print, we could quote\nit to other publications, and claim that with 1000 users we had 20%\nof the online store market.This was roughly true.  We really did have the biggest share of the\nonline store market, and 5000 was our best guess at its size.  But\nthe way the story appeared in the press sounded a lot more definite.Reporters like definitive statements.  For example, many of the\nstories about Jeremy Jaynes's conviction say that he was one of the\n10 worst spammers.  This \"fact\" originated in Spamhaus's ROKSO list,\nwhich I think even Spamhaus would admit is a rough guess at the top\nspammers.  The first stories about Jaynes cited this source, but\nnow it's simply repeated as if it were part of the indictment.   \n[4]All you can say with certainty about Jaynes is that he was a fairly\nbig spammer.  But reporters don't want to print vague stuff like\n\"fairly big.\"  They want statements with punch, like \"top ten.\" And\nPR firms give them what they want.\nWearing suits, we're told, will make us \n3.6\npercent more productive.BuzzWhere the work of PR firms really does get deliberately misleading is in\nthe generation of \"buzz.\"  They usually feed the same story to    \nseveral different publications at once.  And when readers see similar\nstories in multiple places, they think there is some important trend\nafoot.  Which is exactly what they're supposed to think.When Windows 95 was launched, people waited outside stores\nat midnight to buy the first copies.  None of them would have been\nthere without PR firms, who generated such a buzz in\nthe news media that it became self-reinforcing, like a nuclear chain\nreaction.I doubt PR firms realize it yet, but the Web makes it possible to  \ntrack them at work.  If you search for the obvious phrases, you\nturn up several efforts over the years to place stories about the  \nreturn of the suit.  For example, the Reuters article \n\nthat got picked up by USA\nToday in September 2004.  \"The suit is back,\" it begins.Trend articles like this are almost always the work of\nPR firms.  Once you know how to read them, it's straightforward to\nfigure out who the client is.  With trend stories, PR firms usually\nline up one or more \"experts\" to talk about the industry generally. \nIn this case we get three: the NPD Group, the creative director of\nGQ, and a research director at Smith Barney.  [5] When\nyou get to the end of the experts, look for the client. And bingo, \nthere it is: The Men's Wearhouse.Not surprising, considering The Men's Wearhouse was at that moment \nrunning ads saying \"The Suit is Back.\"  Talk about a successful\npress hit-- a wire service article whose first sentence is your own\nad copy.The secret to finding other press hits from a given pitch\nis to realize that they all started from the same document back at\nthe PR firm.  Search for a few key phrases and the names of the\nclients and the experts, and you'll turn up other variants of this \nstory.Casual\nfridays are out and dress codes are in writes Diane E. Lewis\nin The Boston Globe.  In a remarkable coincidence, Ms. Lewis's\nindustry contacts also include the creative director of GQ.Ripped jeans and T-shirts are out, writes Mary Kathleen Flynn in\nUS News & World Report.  And she too knows the \ncreative director of GQ.Men's suits\nare back writes Nicole Ford in Sexbuzz.Com (\"the ultimate men's\nentertainment magazine\").Dressing\ndown loses appeal as men suit up at the office writes Tenisha\nMercer of The Detroit News.\nNow that so many news articles are online, I suspect you could find\na similar pattern for most trend stories placed by PR firms.  I\npropose we call this new sport \"PR diving,\" and I'm sure there are\nfar more striking examples out there than this clump of five stories.OnlineAfter spending years chasing them, it's now second nature\nto me to recognize press hits for what they are.  But before we\nhired a PR firm I had no idea where articles in the mainstream media\ncame from.  I could tell a lot of them were crap, but I didn't\nrealize why.Remember the exercises in critical reading you did in school, where\nyou had to look at a piece of writing and step back and ask whether\nthe author was telling the whole truth?  If you really want to be\na critical reader, it turns out you have to step back one step\nfurther, and ask not just whether the author is telling the truth,\nbut why he's writing about this subject at all.Online, the answer tends to be a lot simpler.  Most people who\npublish online write what they write for the simple reason that\nthey want to.  You\ncan't see the fingerprints of PR firms all over the articles, as\nyou can in so many print publications-- which is one of the reasons,\nthough they may not consciously realize it, that readers trust\nbloggers more than Business Week.I was talking recently to a friend who works for a\nbig newspaper.  He thought the print media were in serious trouble,\nand that they were still mostly in denial about it.  \"They think\nthe decline is cyclic,\" he said.  \"Actually it's structural.\"In other words, the readers are leaving, and they're not coming\nback.\nWhy? I think the main reason is that the writing online is more honest.\nImagine how incongruous the New York Times article about\nsuits would sound if you read it in a blog:\n   The urge to look corporate-- sleek, commanding,\n  prudent, yet with just a touch of hubris on your well-cut sleeve--\n  is an unexpected development in a time of business disgrace.\n   \nThe problem\nwith this article is not just that it originated in a PR firm.\nThe whole tone is bogus.  This is the tone of someone writing down\nto their audience.Whatever its flaws, the writing you find online\nis authentic.  It's not mystery meat cooked up\nout of scraps of pitch letters and press releases, and pressed into \nmolds of zippy\njournalese.  It's people writing what they think.I didn't realize, till there was an alternative, just how artificial\nmost of the writing in the mainstream media was.  I'm not saying\nI used to believe what I read in Time and Newsweek.  Since high\nschool, at least, I've thought of magazines like that more as\nguides to what ordinary people were being\ntold to think than as  \nsources of information.  But I didn't realize till the last  \nfew years that writing for publication didn't have to mean writing\nthat way.  I didn't realize you could write as candidly and\ninformally as you would if you were writing to a friend.Readers aren't the only ones who've noticed the\nchange.  The PR industry has too.\nA hilarious article\non the site of the PR Society of America gets to the heart of the   \nmatter:\n   Bloggers are sensitive about becoming mouthpieces\n  for other organizations and companies, which is the reason they\n  began blogging in the first place.  \nPR people fear bloggers for the same reason readers\nlike them.  And that means there may be a struggle ahead.  As\nthis new kind of writing draws readers away from traditional media, we\nshould be prepared for whatever PR mutates into to compensate.  \nWhen I think   \nhow hard PR firms work to score press hits in the traditional   \nmedia, I can't imagine they'll work any less hard to feed stories\nto bloggers, if they can figure out how.\nNotes[1] PR has at least   \none beneficial feature: it favors small companies.  If PR didn't  \nwork, the only alternative would be to advertise, and only big\ncompanies can afford that.[2] Advertisers pay \nless for ads in free publications, because they assume readers \nignore something they get for free.  This is why so many trade\npublications nominally have a cover price and yet give away free\nsubscriptions with such abandon.[3] Different sections\nof the Times vary so much in their standards that they're\npractically different papers.  Whoever fed the style section reporter\nthis story about suits coming back would have been sent packing by\nthe regular news reporters.[4] The most striking\nexample I know of this type is the \"fact\" that the Internet worm   \nof 1988 infected 6000 computers. I was there when it was cooked up,\nand this was the recipe: someone guessed that there were about\n60,000 computers attached to the Internet, and that the worm might\nhave infected ten percent of them.Actually no one knows how many computers the worm infected, because\nthe remedy was to reboot them, and this destroyed all traces.  But\npeople like numbers.  And so this one is now replicated\nall over the Internet, like a little worm of its own.[5] Not all were\nnecessarily supplied by the PR firm. Reporters sometimes call a few\nadditional sources on their own, like someone adding a few fresh \nvegetables to a can of soup.\nThanks to Ingrid Basset, Trevor Blackwell, Sarah Harlin, Jessica \nLivingston, Jackie McDonough, Robert Morris, and Aaron Swartz (who\nalso found the PRSA article) for reading drafts of this.Correction: Earlier versions used a recent\nBusiness Week article mentioning del.icio.us as an example\nof a press hit, but Joshua Schachter tells me \nit was spontaneous.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nOctober 2010\n\n(I wrote this for Forbes, who asked me to write something\nabout the qualities we look for in founders.  In print they had to cut\nthe last item because they didn't have room.)1. DeterminationThis has turned out to be the most important quality in startup\nfounders.  We thought when we started Y Combinator that the most\nimportant quality would be intelligence.  That's the myth in the\nValley. And certainly you don't want founders to be stupid.  But\nas long as you're over a certain threshold of intelligence, what\nmatters most is determination.  You're going to hit a lot of\nobstacles.  You can't be the sort of person who gets demoralized\neasily.Bill Clerico and Rich Aberman of WePay \nare a good example.  They're\ndoing a finance startup, which means endless negotiations with big,\nbureaucratic companies.  When you're starting a startup that depends\non deals with big companies to exist, it often feels like they're\ntrying to ignore you out of existence.  But when Bill Clerico starts\ncalling you, you may as well do what he asks, because he is not\ngoing away.\n2. FlexibilityYou do not however want the sort of determination implied by phrases\nlike \"don't give up on your dreams.\"  The world of startups is so\nunpredictable that you need to be able to modify your dreams on the\nfly.  The best metaphor I've found for the combination of determination\nand flexibility you need is a running back.  \nHe's determined to get\ndownfield, but at any given moment he may need to go sideways or\neven backwards to get there.The current record holder for flexibility may be Daniel Gross of\nGreplin.  He applied to YC with \nsome bad ecommerce idea.  We told\nhim we'd fund him if he did something else.  He thought for a second,\nand said ok.  He then went through two more ideas before settling\non Greplin.  He'd only been working on it for a couple days when\nhe presented to investors at Demo Day, but he got a lot of interest.\nHe always seems to land on his feet.\n3. ImaginationIntelligence does matter a lot of course.  It seems like the type\nthat matters most is imagination.  It's not so important to be able\nto solve predefined problems quickly as to be able to come up with\nsurprising new ideas.  In the startup world, most good ideas \nseem\nbad initially.  If they were obviously good, someone would already\nbe doing them.  So you need the kind of intelligence that produces\nideas with just the right level of craziness.Airbnb is that kind of idea.  \nIn fact, when we funded Airbnb, we\nthought it was too crazy.  We couldn't believe large numbers of\npeople would want to stay in other people's places.  We funded them\nbecause we liked the founders so much.  As soon as we heard they'd\nbeen supporting themselves by selling Obama and McCain branded\nbreakfast cereal, they were in.  And it turned out the idea was on\nthe right side of crazy after all.\n4. NaughtinessThough the most successful founders are usually good people, they\ntend to have a piratical gleam in their eye.  They're not Goody\nTwo-Shoes type good.  Morally, they care about getting the big\nquestions right, but not about observing proprieties.  That's why\nI'd use the word naughty rather than evil.  They delight in \nbreaking\nrules, but not rules that matter.  This quality may be redundant\nthough; it may be implied by imagination.Sam Altman of Loopt \nis one of the most successful alumni, so we\nasked him what question we could put on the Y Combinator application\nthat would help us discover more people like him.  He said to ask\nabout a time when they'd hacked something to their advantage\u2014hacked in the sense of beating the system, not breaking into\ncomputers.  It has become one of the questions we pay most attention\nto when judging applications.\n5. FriendshipEmpirically it seems to be hard to start a startup with just \none\nfounder.  Most of the big successes have two or three.  And the\nrelationship between the founders has to be strong.  They must\ngenuinely like one another, and work well together.  Startups do\nto the relationship between the founders what a dog does to a sock:\nif it can be pulled apart, it will be.Emmett Shear and Justin Kan of Justin.tv \nare a good example of close\nfriends who work well together.  They've known each other since\nsecond grade.  They can practically read one another's minds.  I'm\nsure they argue, like all founders, but I have never once sensed\nany unresolved tension between them.Thanks to Jessica Livingston and Chris Steiner for reading drafts of this.December 2019There are two distinct ways to be politically moderate: on purpose\nand by accident. Intentional moderates are trimmers, deliberately\nchoosing a position mid-way between the extremes of right and left.\nAccidental moderates end up in the middle, on average, because they\nmake up their own minds about each question, and the far right and\nfar left are roughly equally wrong.You can distinguish intentional from accidental moderates by the\ndistribution of their opinions. If the far left opinion on some\nmatter is 0 and the far right opinion 100, an intentional moderate's\nopinion on every question will be near 50. Whereas an accidental\nmoderate's opinions will be scattered over a broad range, but will,\nlike those of the intentional moderate, average to about 50.Intentional moderates are similar to those on the far left and the\nfar right in that their opinions are, in a sense, not their own.\nThe defining quality of an ideologue, whether on the left or the\nright, is to acquire one's opinions in bulk. You don't get to pick\nand choose. Your opinions about taxation can be predicted from your\nopinions about sex. And although intentional moderates\nmight seem to be the opposite of ideologues, their beliefs (though\nin their case the word \"positions\" might be more accurate) are also\nacquired in bulk. If the median opinion shifts to the right or left,\nthe intentional moderate must shift with it. Otherwise they stop\nbeing moderate.Accidental moderates, on the other hand, not only choose their own\nanswers, but choose their own questions. They may not care at all\nabout questions that the left and right both think are terribly\nimportant. So you can only even measure the politics of an accidental\nmoderate from the intersection of the questions they care about and\nthose the left and right care about, and this can\nsometimes be vanishingly small.It is not merely a manipulative rhetorical trick to say \"if you're\nnot with us, you're against us,\" but often simply false.Moderates are sometimes derided as cowards, particularly by \nthe extreme left. But while it may be accurate to call intentional\nmoderates cowards, openly being an accidental moderate requires the\nmost courage of all, because you get attacked from both right and\nleft, and you don't have the comfort of being an orthodox member\nof a large group to sustain you.Nearly all the most impressive people I know are accidental moderates.\nIf I knew a lot of professional athletes, or people in the entertainment\nbusiness, that might be different. Being on the far left or far\nright doesn't affect how fast you run or how well you sing. But\nsomeone who works with ideas has to be independent-minded to do it\nwell.Or more precisely, you have to be independent-minded about the ideas\nyou work with. You could be mindlessly doctrinaire in your politics\nand still be a good mathematician. In the 20th century, a lot of\nvery smart people were Marxists \u0097 just no one who was smart about\nthe subjects Marxism involves. But if the ideas you use in your\nwork intersect with the politics of your time, you have two choices:\nbe an accidental moderate, or be mediocre.Notes[1] It's possible in theory for one side to be entirely right and\nthe other to be entirely wrong. Indeed, ideologues must always\nbelieve this is the case. But historically it rarely has been.[2] For some reason the far right tend to ignore moderates rather\nthan despise them as backsliders. I'm not sure why. Perhaps it\nmeans that the far right is less ideological than the far left. Or\nperhaps that they are more confident, or more resigned, or simply\nmore disorganized. I just don't know.[3] Having heretical opinions doesn't mean you have to express\nthem openly. It may be\neasier to have them if you don't.\nThanks to Austen Allred, Trevor Blackwell, Patrick Collison, Jessica Livingston,\nAmjad Masad, Ryan Petersen, and Harj Taggar for reading drafts of this.January 2016Life is short, as everyone knows. When I was a kid I used to wonder\nabout this. Is life actually short, or are we really complaining\nabout its finiteness?  Would we be just as likely to feel life was\nshort if we lived 10 times as long?Since there didn't seem any way to answer this question, I stopped\nwondering about it.  Then I had kids.  That gave me a way to answer\nthe question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,\ninto discrete quantities. You only get 52 weekends with your 2 year\nold.  If Christmas-as-magic lasts from say ages 3 to 10, you only\nget to watch your child experience it 8 times.  And while it's\nimpossible to say what is a lot or a little of a continuous quantity\nlike time, 8 is not a lot of something.  If you had a handful of 8\npeanuts, or a shelf of 8 books to choose from, the quantity would\ndefinitely seem limited, no matter what your lifespan was.Ok, so life actually is short.  Does it make any difference to know\nthat?It has for me.  It means arguments of the form \"Life is too short\nfor x\" have great force.  It's not just a figure of speech to say\nthat life is too short for something.  It's not just a synonym for\nannoying.  If you find yourself thinking that life is too short for\nsomething, you should try to eliminate it if you can.When I ask myself what I've found life is too short for, the word\nthat pops into my head is \"bullshit.\" I realize that answer is\nsomewhat tautological.  It's almost the definition of bullshit that\nit's the stuff that life is too short for.  And yet bullshit does\nhave a distinctive character.  There's something fake about it.\nIt's the junk food of experience.\n[1]If you ask yourself what you spend your time on that's bullshit,\nyou probably already know the answer.  Unnecessary meetings, pointless\ndisputes, bureaucracy, posturing, dealing with other people's\nmistakes, traffic jams, addictive but unrewarding pastimes.There are two ways this kind of thing gets into your life: it's\neither forced on you, or it tricks you.  To some extent you have to\nput up with the bullshit forced on you by circumstances.  You need\nto make money, and making money consists mostly of errands.  Indeed,\nthe law of supply and demand insures that: the more rewarding some\nkind of work is, the cheaper people will do it.  It may be that\nless bullshit is forced on you than you think, though.  There has\nalways been a stream of people who opt out of the default grind and\ngo live somewhere where opportunities are fewer in the conventional\nsense, but life feels more authentic.  This could become more common.You can do it on a smaller scale without moving.  The amount of\ntime you have to spend on bullshit varies between employers.  Most\nlarge organizations (and many small ones) are steeped in it.  But\nif you consciously prioritize bullshit avoidance over other factors\nlike money and prestige, you can probably find employers that will\nwaste less of your time.If you're a freelancer or a small company, you can do this at the\nlevel of individual customers.  If you fire or avoid toxic customers,\nyou can decrease the amount of bullshit in your life by more than\nyou decrease your income.But while some amount of bullshit is inevitably forced on you, the\nbullshit that sneaks into your life by tricking you is no one's\nfault but your own.  And yet the bullshit you choose may be harder\nto eliminate than the bullshit that's forced on you.  Things that\nlure you into wasting your time have to be really good at\ntricking you.  An example that will be familiar to a lot of people\nis arguing online.  When someone\ncontradicts you, they're in a sense attacking you. Sometimes pretty\novertly.  Your instinct when attacked is to defend yourself.  But\nlike a lot of instincts, this one wasn't designed for the world we\nnow live in.  Counterintuitive as it feels, it's better most of\nthe time not to defend yourself.  Otherwise these people are literally\ntaking your life.\n[2]Arguing online is only incidentally addictive. There are more\ndangerous things than that. As I've written before, one byproduct\nof technical progress is that things we like tend to become more\naddictive.  Which means we will increasingly have to make a conscious\neffort to avoid addictions \u0097 to stand outside ourselves and ask \"is\nthis how I want to be spending my time?\"As well as avoiding bullshit, one should actively seek out things\nthat matter.  But different things matter to different people, and\nmost have to learn what matters to them.  A few are lucky and realize\nearly on that they love math or taking care of animals or writing,\nand then figure out a way to spend a lot of time doing it.  But\nmost people start out with a life that's a mix of things that\nmatter and things that don't, and only gradually learn to distinguish\nbetween them.For the young especially, much of this confusion is induced by the\nartificial situations they find themselves in. In middle school and\nhigh school, what the other kids think of you seems the most important\nthing in the world.  But when you ask adults what they got wrong\nat that age, nearly all say they cared too much what other kids\nthought of them.One heuristic for distinguishing stuff that matters is to ask\nyourself whether you'll care about it in the future.  Fake stuff\nthat matters usually has a sharp peak of seeming to matter.  That's\nhow it tricks you.  The area under the curve is small, but its shape\njabs into your consciousness like a pin.The things that matter aren't necessarily the ones people would\ncall \"important.\"  Having coffee with a friend matters.  You won't\nfeel later like that was a waste of time.One great thing about having small children is that they make you\nspend time on things that matter: them. They grab your sleeve as\nyou're staring at your phone and say \"will you play with me?\" And\nodds are that is in fact the bullshit-minimizing option.If life is short, we should expect its shortness to take us by\nsurprise. And that is just what tends to happen.  You take things\nfor granted, and then they're gone.  You think you can always write\nthat book, or climb that mountain, or whatever, and then you realize\nthe window has closed.  The saddest windows close when other people\ndie. Their lives are short too.  After my mother died, I wished I'd\nspent more time with her.  I lived as if she'd always be there.\nAnd in her typical quiet way she encouraged that illusion.  But an\nillusion it was. I think a lot of people make the same mistake I\ndid.The usual way to avoid being taken by surprise by something is to\nbe consciously aware of it.  Back when life was more precarious,\npeople used to be aware of death to a degree that would now seem a\nbit morbid.  I'm not sure why, but it doesn't seem the right answer\nto be constantly reminding oneself of the grim reaper hovering at\neveryone's shoulder.  Perhaps a better solution is to look at the\nproblem from the other end. Cultivate a habit of impatience about\nthe things you most want to do. Don't wait before climbing that\nmountain or writing that book or visiting your mother.  You don't\nneed to be constantly reminding yourself why you shouldn't wait.\nJust don't wait.I can think of two more things one does when one doesn't have much\nof something: try to get more of it, and savor what one has.  Both\nmake sense here.How you live affects how long you live.  Most people could do better.\nMe among them.But you can probably get even more effect by paying closer attention\nto the time you have.  It's easy to let the days rush by.  The\n\"flow\" that imaginative people love so much has a darker cousin\nthat prevents you from pausing to savor life amid the daily slurry\nof errands and alarms.  One of the most striking things I've read\nwas not in a book, but the title of one: James Salter's Burning\nthe Days.It is possible to slow time somewhat. I've gotten better at it.\nKids help.  When you have small children, there are a lot of moments\nso perfect that you can't help noticing.It does help too to feel that you've squeezed everything out of\nsome experience.  The reason I'm sad about my mother is not just\nthat I miss her but that I think of all the things we could have\ndone that we didn't.  My oldest son will be 7 soon.  And while I\nmiss the 3 year old version of him, I at least don't have any regrets\nover what might have been.  We had the best time a daddy and a 3\nyear old ever had.Relentlessly prune bullshit, don't wait to do things that matter,\nand savor the time you have.  That's what you do when life is short.Notes[1]\nAt first I didn't like it that the word that came to mind was\none that had other meanings.  But then I realized the other meanings\nare fairly closely related.  Bullshit in the sense of things you\nwaste your time on is a lot like intellectual bullshit.[2]\nI chose this example deliberately as a note to self.  I get\nattacked a lot online.  People tell the craziest lies about me.\nAnd I have so far done a pretty mediocre job of suppressing the\nnatural human inclination to say \"Hey, that's not true!\"Thanks to Jessica Livingston and Geoff Ralston for reading drafts\nof this.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nNovember 2009I don't think Apple realizes how badly the App Store approval process\nis broken.  Or rather, I don't think they realize how much it matters\nthat it's broken.The way Apple runs the App Store has harmed their reputation with\nprogrammers more than anything else they've ever done. \nTheir reputation with programmers used to be great.\nIt used to be the most common complaint you heard\nabout Apple was that their fans admired them too uncritically.\nThe App Store has changed that.  Now a lot of programmers\nhave started to see Apple as evil.How much of the goodwill Apple once had with programmers have they\nlost over the App Store?  A third?  Half?  And that's just so far.\nThe App Store is an ongoing karma leak.* * *How did Apple get into this mess?  Their fundamental problem is\nthat they don't understand software.They treat iPhone apps the way they treat the music they sell through\niTunes.  Apple is the channel; they own the user; if you want to\nreach users, you do it on their terms. The record labels agreed,\nreluctantly.  But this model doesn't work for software.  It doesn't\nwork for an intermediary to own the user.  The software business\nlearned that in the early 1980s, when companies like VisiCorp showed\nthat although the words \"software\" and \"publisher\" fit together,\nthe underlying concepts don't.  Software isn't like music or books.\nIt's too complicated for a third party to act as an intermediary\nbetween developer and user.   And yet that's what Apple is trying\nto be with the App Store: a software publisher.  And a particularly\noverreaching one at that, with fussy tastes and a rigidly enforced\nhouse style.If software publishing didn't work in 1980, it works even less now\nthat software development has evolved from a small number of big\nreleases to a constant stream of small ones.  But Apple doesn't\nunderstand that either.  Their model of product development derives\nfrom hardware.  They work on something till they think it's finished,\nthen they release it.  You have to do that with hardware, but because\nsoftware is so easy to change, its design can benefit from evolution.\nThe standard way to develop applications now is to launch fast and\niterate.  Which means it's a disaster to have long, random delays\neach time you release a new version.Apparently Apple's attitude is that developers should be more careful\nwhen they submit a new version to the App Store.  They would say\nthat.  But powerful as they are, they're not powerful enough to\nturn back the evolution of technology.  Programmers don't use\nlaunch-fast-and-iterate out of laziness.  They use it because it\nyields the best results.  By obstructing that process, Apple is\nmaking them do bad work, and programmers hate that as much as Apple\nwould.How would Apple like it if when they discovered a serious bug in\nOS\u00a0X, instead of releasing a software update immediately, they had\nto submit their code to an intermediary who sat on it for a month\nand then rejected it because it contained an icon they didn't like?By breaking software development, Apple gets the opposite of what\nthey intended: the version of an app currently available in the App\nStore tends to be an old and buggy one.  One developer told me:\n\n  As a result of their process, the App Store is full of half-baked\n  applications. I make a new version almost every day that I release\n  to beta users. The version on the App Store feels old and crappy.\n  I'm sure that a lot of developers feel this way: One emotion is\n  \"I'm not really proud about what's in the App Store\", and it's\n  combined with the emotion \"Really, it's Apple's fault.\"\n\nAnother wrote:\n\n  I believe that they think their approval process helps users by\n  ensuring quality.  In reality, bugs like ours get through all the\n  time and then it can take 4-8 weeks to get that bug fix approved,\n  leaving users to think that iPhone apps sometimes just don't work.\n  Worse for Apple, these apps work just fine on other platforms\n  that have immediate approval processes.\n\nActually I suppose Apple has a third misconception: that all the\ncomplaints about App Store approvals are not a serious problem.\nThey must hear developers complaining.  But partners and suppliers\nare always complaining.  It would be a bad sign if they weren't;\nit would mean you were being too easy on them.  Meanwhile the iPhone\nis selling better than ever.  So why do they need to fix anything?They get away with maltreating developers, in the short term, because\nthey make such great hardware.  I just bought a new 27\" iMac a\ncouple days ago.  It's fabulous.  The screen's too shiny, and the\ndisk is surprisingly loud, but it's so beautiful that you can't\nmake yourself care.So I bought it, but I bought it, for the first time, with misgivings.\nI felt the way I'd feel buying something made in a country with a\nbad human rights record.  That was new.  In the past when I bought\nthings from Apple it was an unalloyed pleasure.  Oh boy!  They make\nsuch great stuff.  This time it felt like a Faustian bargain.  They\nmake such great stuff, but they're such assholes.  Do I really want\nto support this company?* * *Should Apple care what people like me think?  What difference does\nit make if they alienate a small minority of their users?There are a couple reasons they should care.  One is that these\nusers are the people they want as employees.  If your company seems\nevil, the best programmers won't work for you.  That hurt Microsoft\na lot starting in the 90s.  Programmers started to feel sheepish\nabout working there.  It seemed like selling out.  When people from\nMicrosoft were talking to other programmers and they mentioned where\nthey worked, there were a lot of self-deprecating jokes about having\ngone over to the dark side.  But the real problem for Microsoft\nwasn't the embarrassment of the people they hired.  It was the\npeople they never got.  And you know who got them?  Google and\nApple.  If Microsoft was the Empire, they were the Rebel Alliance.\nAnd it's largely because they got more of the best people that\nGoogle and Apple are doing so much better than Microsoft today.Why are programmers so fussy about their employers' morals?  Partly\nbecause they can afford to be.  The best programmers can work\nwherever they want.  They don't have to work for a company they\nhave qualms about.But the other reason programmers are fussy, I think, is that evil\nbegets stupidity.  An organization that wins by exercising power\nstarts to lose the ability to win by doing better work.  And it's\nnot fun for a smart person to work in a place where the best ideas\naren't the ones that win.  I think the reason Google embraced \"Don't\nbe evil\" so eagerly was not so much to impress the outside world\nas to inoculate themselves against arrogance.\n[1]That has worked for Google so far.  They've become more\nbureaucratic, but otherwise they seem to have held true to their\noriginal principles. With Apple that seems less the case.\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n  When you\nlook at the famous \n1984 ad \nnow, it's easier to imagine Apple as the\ndictator on the screen than the woman with the hammer.\n[2]\nIn fact, if you read the dictator's speech it sounds uncannily like a\nprophecy of the App Store.\n\n  We have triumphed over the unprincipled dissemination of facts.We have created, for the first time in all history, a garden of\n  pure ideology, where each worker may bloom secure from the pests\n  of contradictory and confusing truths.\n\nThe other reason Apple should care what programmers think of them\nis that when you sell a platform, developers make or break you.  If\nanyone should know this, Apple should.  VisiCalc made the Apple II.And programmers build applications for the platforms they use.  Most\napplications\u2014most startups, probably\u2014grow out of personal projects.\nApple itself did.  Apple made microcomputers because that's what\nSteve Wozniak wanted for himself.  He couldn't have afforded a\nminicomputer. \n[3]\n Microsoft likewise started out making interpreters\nfor little microcomputers because\nBill Gates and Paul Allen were interested in using them.  It's a\nrare startup that doesn't build something the founders use.The main reason there are so many iPhone apps is that so many programmers\nhave iPhones.  They may know, because they read it in an article,\nthat Blackberry has such and such market share.  But in practice\nit's as if RIM didn't exist. If they're going to build something,\nthey want to be able to use it themselves, and that means building\nan iPhone app.So programmers continue to develop iPhone apps, even though Apple\ncontinues to maltreat them.  They're like someone stuck in an abusive\nrelationship.  They're so attracted to the iPhone that they can't\nleave.  But they're looking for a way out.  One wrote:\n\n  While I did enjoy developing for the iPhone, the control they\n  place on the App Store does not give me the drive to develop\n  applications as I would like. In fact I don't intend to make any\n  more iPhone applications unless absolutely necessary.\n[4]\n\nCan anything break this cycle?  No device I've seen so far could.\nPalm and RIM haven't a hope.  The only credible contender is Android.\nBut Android is an orphan; Google doesn't really care about it, not\nthe way Apple cares about the iPhone.  Apple cares about the iPhone\nthe way Google cares about search.* * *Is the future of handheld devices one locked down by Apple?  It's\na worrying prospect.  It would be a bummer to have another grim\nmonoculture like we had in the 1990s.  In 1995, writing software\nfor end users was effectively identical with writing Windows\napplications.  Our horror at that prospect was the single biggest\nthing that drove us to start building web apps.At least we know now what it would take to break Apple's lock.\nYou'd have to get iPhones out of programmers' hands.  If programmers\nused some other device for mobile web access, they'd start to develop\napps for that instead.How could you make a device programmers liked better than the iPhone?\nIt's unlikely you could make something better designed.  Apple\nleaves no room there.  So this alternative device probably couldn't\nwin on general appeal.  It would have to win by virtue of some\nappeal it had to programmers specifically.One way to appeal to programmers is with software.  If you\ncould think of an application programmers had to have, but that\nwould be impossible in the circumscribed world of the iPhone, \nyou could presumably get them to switch.That would definitely happen if programmers started to use handhelds\nas development machines\u2014if handhelds displaced laptops the\nway laptops displaced desktops.  You need more control of a development\nmachine than Apple will let you have over an iPhone.Could anyone make a device that you'd carry around in your pocket\nlike a phone, and yet would also work as a development machine?\nIt's hard to imagine what it would look like.  But I've learned\nnever to say never about technology.  A phone-sized device that\nwould work as a development machine is no more miraculous by present\nstandards than the iPhone itself would have seemed by the standards\nof 1995.My current development machine is a MacBook Air, which I use with\nan external monitor and keyboard in my office, and by itself when\ntraveling.  If there was a version half the size I'd prefer it.\nThat still wouldn't be small enough to carry around everywhere like\na phone, but we're within a factor of 4 or so.  Surely that gap is\nbridgeable.  In fact, let's make it an\nRFS. Wanted: \nWoman with hammer.Notes[1]\nWhen Google adopted \"Don't be evil,\" they were still so small\nthat no one would have expected them to be, yet.\n[2]\nThe dictator in the 1984 ad isn't Microsoft, incidentally;\nit's IBM.  IBM seemed a lot more frightening in those days, but\nthey were friendlier to developers than Apple is now.[3]\nHe couldn't even afford a monitor.  That's why the Apple\nI used a TV as a monitor.[4]\nSeveral people I talked to mentioned how much they liked the\niPhone SDK.  The problem is not Apple's products but their policies.\nFortunately policies are software; Apple can change them instantly\nif they want to.  Handy that, isn't it?Thanks to Sam Altman, Trevor Blackwell, Ross Boucher, \nJames Bracy, Gabor Cselle,\nPatrick Collison, Jason Freedman, John Gruber, Joe Hewitt, Jessica Livingston,\nRobert Morris, Teng Siong Ong, Nikhil Pandit, Savraj Singh, and Jared Tame for reading drafts of this.May 2003If Lisp is so great, why don't more people use it?  I was    \nasked this question by a student in the audience at a \ntalk I gave recently.  Not for the first time, either.In languages, as in so many things, there's not much     \ncorrelation between popularity and quality.  Why does   \nJohn Grisham (King of Torts sales rank, 44) outsell\nJane Austen (Pride and Prejudice sales rank, 6191)?\nWould even Grisham claim that it's because he's a better\nwriter?Here's the first sentence of Pride and Prejudice:\n\nIt is a truth universally acknowledged, that a single man \nin possession of a good fortune must be in want of a\nwife.\n\n\"It is a truth universally acknowledged?\"  Long words for\nthe first sentence of a love story.Like Jane Austen, Lisp looks hard.  Its syntax, or lack\nof syntax, makes it look completely unlike \nthe languages\nmost people are used to.  Before I learned Lisp, I was afraid\nof it too.  I recently came across a notebook from 1983\nin which I'd written:\n\nI suppose I should learn Lisp, but it seems so foreign.\n\nFortunately, I was 19 at the time and not too resistant to learning\nnew things.  I was so ignorant that learning\nalmost anything meant learning new things.People frightened by Lisp make up other reasons for not\nusing it.  The standard\nexcuse, back when C was the default language, was that Lisp\nwas too slow.  Now that Lisp dialects are among\nthe faster\nlanguages available, that excuse has gone away.\nNow the standard excuse is openly circular: that other languages\nare more popular.(Beware of such reasoning.  It gets you Windows.)Popularity is always self-perpetuating, but it's especially\nso in programming languages. More libraries\nget written for popular languages, which makes them still\nmore popular.  Programs often have to work with existing programs,\nand this is easier if they're written in the same language,\nso languages spread from program to program like a virus.\nAnd managers prefer popular languages, because they give them \nmore leverage over developers, who can more easily be replaced.Indeed, if programming languages were all more or less equivalent,\nthere would be little justification for using any but the most\npopular.  But they aren't all equivalent, not by a long\nshot.  And that's why less popular languages, like Jane Austen's \nnovels, continue to survive at all.  When everyone else is reading \nthe latest John Grisham novel, there will always be a few people \nreading Jane Austen instead.October 2015This will come as a surprise to a lot of people, but in some cases\nit's possible to detect bias in a selection process without knowing\nanything about the applicant pool.  Which is exciting because among\nother things it means third parties can use this technique to detect\nbias whether those doing the selecting want them to or not.You can use this technique whenever (a) you have at least\na random sample of the applicants that were selected, (b) their\nsubsequent performance is measured, and (c) the groups of\napplicants you're comparing have roughly equal distribution of ability.How does it work?  Think about what it means to be biased.  What\nit means for a selection process to be biased against applicants\nof type x is that it's harder for them to make it through.  Which\nmeans applicants of type x have to be better to get selected than\napplicants not of type x.\n[1]\nWhich means applicants of type x\nwho do make it through the selection process will outperform other\nsuccessful applicants.  And if the performance of all the successful\napplicants is measured, you'll know if they do.Of course, the test you use to measure performance must be a valid\none.  And in particular it must not be invalidated by the bias you're\ntrying to measure.\nBut there are some domains where performance can be measured, and\nin those detecting bias is straightforward. Want to know if the\nselection process was biased against some type of applicant?  Check\nwhether they outperform the others.  This is not just a heuristic\nfor detecting bias.  It's what bias means.For example, many suspect that venture capital firms are biased\nagainst female founders. This would be easy to detect: among their\nportfolio companies, do startups with female founders outperform\nthose without?  A couple months ago, one VC firm (almost certainly\nunintentionally) published a study showing bias of this type. First\nRound Capital found that among its portfolio companies, startups\nwith female founders outperformed\nthose without by 63%. \n[2]The reason I began by saying that this technique would come as a\nsurprise to many people is that we so rarely see analyses of this\ntype.  I'm sure it will come as a surprise to First Round that they\nperformed one. I doubt anyone there realized that by limiting their\nsample to their own portfolio, they were producing a study not of\nstartup trends but of their own biases when selecting companies.I predict we'll see this technique used more in the future.  The\ninformation needed to conduct such studies is increasingly available.\nData about who applies for things is usually closely guarded by the\norganizations selecting them, but nowadays data about who gets\nselected is often publicly available to anyone who takes the trouble\nto aggregate it.\nNotes[1]\nThis technique wouldn't work if the selection process looked\nfor different things from different types of applicants\u2014for\nexample, if an employer hired men based on their ability but women\nbased on their appearance.[2]\nAs Paul Buchheit points out, First Round excluded their most \nsuccessful investment, Uber, from the study.  And while it \nmakes sense to exclude outliers from some types of studies, \nstudies of returns from startup investing, which is all about \nhitting outliers, are not one of them.\nThanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading\ndrafts of this.January 2017People who are powerful but uncharismatic will tend to be disliked.\nTheir power makes them a target for criticism that they don't have\nthe charisma to disarm. That was Hillary Clinton's problem. It also\ntends to be a problem for any CEO who is more of a builder than a\nschmoozer. And yet the builder-type CEO is (like Hillary) probably\nthe best person for the job.I don't think there is any solution to this problem. It's human\nnature. The best we can do is to recognize that it's happening, and\nto understand that being a magnet for criticism is sometimes a sign\nnot that someone is the wrong person for a job, but that they're\nthe right one.July 2006I've discovered a handy test for figuring out what you're addicted\nto.  Imagine you were going to spend the weekend at a friend's house\non a little island off the coast of Maine.  There are no shops on\nthe island and you won't be able to leave while you're there.  Also,\nyou've never been to this house before, so you can't assume it will\nhave more than any house might.What, besides clothes and toiletries, do you make a point of packing?\nThat's what you're addicted to.  For example, if you find yourself\npacking a bottle of vodka (just in case), you may want to stop and\nthink about that.For me the list is four things: books, earplugs, a notebook, and a\npen.There are other things I might bring if I thought of it, like music,\nor tea, but I can live without them.  I'm not so addicted to caffeine\nthat I wouldn't risk the house not having any tea, just for a\nweekend.Quiet is another matter.  I realize it seems a bit eccentric to\ntake earplugs on a trip to an island off the coast of Maine.  If\nanywhere should be quiet, that should.  But what if the person in\nthe next room snored?  What if there was a kid playing basketball?\n(Thump, thump, thump... thump.)  Why risk it?  Earplugs are small.Sometimes I can think with noise.  If I already have momentum on\nsome project, I can work in noisy places.  I can edit an essay or\ndebug code in an airport.  But airports are not so bad: most of the\nnoise is whitish.  I couldn't work with the sound of a sitcom coming\nthrough the wall, or a car in the street playing thump-thump music.And of course there's another kind of thinking, when you're starting\nsomething new, that requires complete quiet.   You never\nknow when this will strike. It's just as well to carry plugs.The notebook and pen are professional equipment, as it were.  Though\nactually there is something druglike about them, in the sense that\ntheir main purpose is to make me feel better.  I hardly ever go\nback and read stuff I write down in notebooks.  It's just that if\nI can't write things down, worrying about remembering one idea gets\nin the way of having the next.  Pen and paper wick ideas.The best notebooks I've found are made by a company called Miquelrius.\nI use their smallest size, which is about 2.5 x 4 in.\nThe secret to writing on such\nnarrow pages is to break words only when you run out of space, like\na Latin inscription.  I use the cheapest plastic Bic ballpoints,\npartly because their gluey ink doesn't seep through pages, and\npartly so I don't worry about losing them.I only started carrying a notebook about three years ago.  Before\nthat I used whatever scraps of paper I could find.  But the problem\nwith scraps of paper is that they're not ordered.  In a notebook\nyou can guess what a scribble means by looking at the pages\naround it.  In the scrap era I was constantly finding notes I'd\nwritten years before that might say something I needed to remember,\nif I could only figure out what.As for books, I know the house would probably have something to\nread.  On the average trip I bring four books and only read one of\nthem, because I find new books to read en route.  Really bringing\nbooks is insurance.I realize this dependence on books is not entirely good\u2014that what\nI need them for is distraction.  The books I bring on trips are\noften quite virtuous, the sort of stuff that might be assigned\nreading in a college class.  But I know my motives aren't virtuous.\nI bring books because if the world gets boring I need to be able\nto slip into another distilled by some writer.  It's like eating\njam when you know you should be eating fruit.There is a point where I'll do without books.  I was walking in\nsome steep mountains once, and decided I'd rather just think, if I\nwas bored, rather than carry a single unnecessary ounce.  It wasn't\nso bad.  I found I could entertain myself by having ideas instead\nof reading other people's.  If you stop eating jam, fruit starts\nto taste better.So maybe I'll try not bringing books on some future trip.  They're\ngoing to have to pry the plugs out of my cold, dead ears, however.February 2007A few days ago I finally figured out something I've wondered about\nfor 25 years: the relationship between wisdom and intelligence.\nAnyone can see they're not the same by the number of people who are\nsmart, but not very wise.  And yet intelligence and wisdom do seem\nrelated.  How?What is wisdom?  I'd say it's knowing what to do in a lot of\nsituations.  I'm not trying to make a deep point here about the\ntrue nature of wisdom, just to figure out how we use the word.  A\nwise person is someone who usually knows the right thing to do.And yet isn't being smart also knowing what to do in certain\nsituations?  For example, knowing what to do when the teacher tells\nyour elementary school class to add all the numbers from 1 to 100?\n[1]Some say wisdom and intelligence apply to different types of\nproblems\u2014wisdom to human problems and intelligence to abstract\nones.  But that isn't true.  Some wisdom has nothing to do with\npeople: for example, the wisdom of the engineer who knows certain\nstructures are less prone to failure than others.  And certainly\nsmart people can find clever solutions to human problems as well\nas abstract ones. \n[2]Another popular explanation is that wisdom comes from experience\nwhile intelligence is innate.  But people are not simply wise in\nproportion to how much experience they have.  Other things must\ncontribute to wisdom besides experience, and some may be innate: a\nreflective disposition, for example.Neither of the conventional explanations of the difference between\nwisdom and intelligence stands up to scrutiny.  So what is the\ndifference?  If we look at how people use the words \"wise\" and\n\"smart,\" what they seem to mean is different shapes of performance.Curve\"Wise\" and \"smart\" are both ways of saying someone knows what to\ndo.  The difference is that \"wise\" means one has a high average\noutcome across all situations, and \"smart\" means one does spectacularly\nwell in a few.  That is, if you had a graph in which the x axis\nrepresented situations and the y axis the outcome, the graph of the\nwise person would be high overall, and the graph of the smart person\nwould have high peaks.The distinction is similar to the rule that one should judge talent\nat its best and character at its worst.  Except you judge intelligence\nat its best, and wisdom by its average.  That's how the two are\nrelated: they're the two different senses in which the same curve\ncan be high.So a wise person knows what to do in most situations, while a smart\nperson knows what to do in situations where few others could.  We\nneed to add one more qualification: we should ignore cases where\nsomeone knows what to do because they have inside information. \n[3]\nBut aside from that, I don't think we can get much more specific\nwithout starting to be mistaken.Nor do we need to.  Simple as it is, this explanation predicts, or\nat least accords with, both of the conventional stories about the\ndistinction between wisdom and intelligence.  Human problems are\nthe most common type, so being good at solving those is key in\nachieving a high average outcome.   And it seems natural that a\nhigh average outcome depends mostly on experience, but that dramatic\npeaks can only be achieved by people with certain rare, innate\nqualities; nearly anyone can learn to be a good swimmer, but to be\nan Olympic swimmer you need a certain body type.This explanation also suggests why wisdom is such an elusive concept:\nthere's no such thing.  \"Wise\" means something\u2014that one is\non average good at making the right choice.  But giving the name\n\"wisdom\" to the supposed quality that enables one to do that doesn't\nmean such a thing exists.  To the extent \"wisdom\" means anything,\nit refers to a grab-bag of qualities as various as self-discipline,\nexperience, and empathy.  \n[4]Likewise, though \"intelligent\" means something, we're asking for\ntrouble if we insist on looking for a single thing called \"intelligence.\"\nAnd whatever its components, they're not all innate.  We use the\nword \"intelligent\" as an indication of ability: a smart person can\ngrasp things few others could.  It does seem likely there's some\ninborn predisposition to intelligence (and wisdom too), but this\npredisposition is not itself intelligence.One reason we tend to think of intelligence as inborn is that people\ntrying to measure it have concentrated on the aspects of it that\nare most measurable.  A quality that's inborn will obviously be\nmore convenient to work with than one that's influenced by experience,\nand thus might vary in the course of a study.  The problem comes\nwhen we drag the word \"intelligence\" over onto what they're measuring.\nIf they're measuring something inborn, they can't be measuring\nintelligence.  Three year olds aren't smart.   When we describe one\nas smart, it's shorthand for \"smarter than other three year olds.\"SplitPerhaps it's a technicality to point out that a predisposition to\nintelligence is not the same as intelligence.  But it's an important\ntechnicality, because it reminds us that we can become smarter,\njust as we can become wiser.The alarming thing is that we may have to choose between the two.If wisdom and intelligence are the average and peaks of the same\ncurve, then they converge as the number of points on the curve\ndecreases.  If there's just one point, they're identical: the average\nand maximum are the same.  But as the number of points increases,\nwisdom and intelligence diverge.  And historically the number of\npoints on the curve seems to have been increasing: our ability is\ntested in an ever wider range of situations.In the time of Confucius and Socrates, people seem to have regarded\nwisdom, learning, and intelligence as more closely related than we\ndo.  Distinguishing between \"wise\" and \"smart\" is a modern habit.\n[5]\nAnd the reason we do is that they've been diverging.  As knowledge\ngets more specialized, there are more points on the curve, and the\ndistinction between the spikes and the average becomes sharper,\nlike a digital image rendered with more pixels.One consequence is that some old recipes may have become obsolete.\nAt the very least we have to go back and figure out if they were\nreally recipes for wisdom or intelligence.  But the really striking\nchange, as intelligence and wisdom drift apart, is that we may have\nto decide which we prefer.  We may not be able to optimize for both\nsimultaneously.Society seems to have voted for intelligence.  We no longer admire\nthe sage\u2014not the way people did two thousand years ago.  Now\nwe admire the genius.  Because in fact the distinction we began\nwith has a rather brutal converse: just as you can be smart without\nbeing very wise, you can be wise without being very smart.  That\ndoesn't sound especially admirable.  That gets you James Bond, who\nknows what to do in a lot of situations, but has to rely on Q for\nthe ones involving math.Intelligence and wisdom are obviously not mutually exclusive.  In\nfact, a high average may help support high peaks.  But there are\nreasons to believe that at some point you have to choose between\nthem.  One is the example of very smart people, who are so often\nunwise that in popular culture this now seems to be regarded as the\nrule rather than the exception.  Perhaps the absent-minded professor\nis wise in his way, or wiser than he seems, but he's not wise in\nthe way Confucius or Socrates wanted people to be. \n[6]NewFor both Confucius and Socrates, wisdom, virtue, and happiness were\nnecessarily related.  The wise man was someone who knew what the\nright choice was and always made it; to be the right choice, it had\nto be morally right; he was therefore always happy, knowing he'd\ndone the best he could.  I can't think of many ancient philosophers\nwho would have disagreed with that, so far as it goes.\"The superior man is always happy; the small man sad,\" said Confucius.\n[7]Whereas a few years ago I read an interview with a mathematician\nwho said that most nights he went to bed discontented, feeling he\nhadn't made enough progress.  \n[8]\nThe Chinese and Greek words we\ntranslate as \"happy\" didn't mean exactly what we do by it, but\nthere's enough overlap that this remark contradicts them.Is the mathematician a small man because he's discontented?  No;\nhe's just doing a kind of work that wasn't very common in Confucius's\nday.Human knowledge seems to grow fractally.  Time after time, something\nthat seemed a small and uninteresting area\u2014experimental error,\neven\u2014turns out, when examined up close, to have as much in\nit as all knowledge up to that point.  Several of the fractal buds\nthat have exploded since ancient times involve inventing and\ndiscovering new things.  Math, for example, used to be something a\nhandful of people did part-time.  Now it's the career of thousands.\nAnd in work that involves making new things, some old rules don't\napply.Recently I've spent some time advising people, and there I find the\nancient rule still works: try to understand the situation as well\nas you can, give the best advice you can based on your experience,\nand then don't worry about it, knowing you did all you could.  But\nI don't have anything like this serenity when I'm writing an essay.\nThen I'm worried.  What if I run out of ideas?  And when I'm writing,\nfour nights out of five I go to bed discontented, feeling I didn't\nget enough done.Advising people and writing are fundamentally different types of\nwork.  When people come to you with a problem and you have to figure\nout the right thing to do, you don't (usually) have to invent\nanything.  You just weigh the alternatives and try to judge which\nis the prudent choice.  But prudence can't tell me what sentence\nto write next.  The search space is too big.Someone like a judge or a military officer can in much of his work\nbe guided by duty, but duty is no guide in making things.  Makers\ndepend on something more precarious: inspiration.  And like most\npeople who lead a precarious existence, they tend to be worried,\nnot contented.  In that respect they're more like the small man of\nConfucius's day, always one bad harvest (or ruler) away from\nstarvation. Except instead of being at the mercy of weather and\nofficials, they're at the mercy of their own imagination.LimitsTo me it was a relief just to realize it might be ok to be discontented.\nThe idea that a successful person should be happy has thousands of\nyears of momentum behind it.  If I was any good, why didn't I have\nthe easy confidence winners are supposed to have?  But that, I now\nbelieve, is like a runner asking \"If I'm such a good athlete, why\ndo I feel so tired?\" Good runners still get tired; they just get\ntired at higher speeds.People whose work is to invent or discover things are in the same\nposition as the runner.  There's no way for them to do the best\nthey can, because there's no limit to what they could do.  The\nclosest you can come is to compare yourself to other people.  But\nthe better you do, the less this matters.  An undergrad who gets\nsomething published feels like a star.  But for someone at the top\nof the field, what's the test of doing well?  Runners can at least\ncompare themselves to others doing exactly the same thing; if you\nwin an Olympic gold medal, you can be fairly content, even if you\nthink you could have run a bit faster.  But what is a novelist to\ndo?Whereas if you're doing the kind of work in which problems are\npresented to you and you have to choose between several alternatives,\nthere's an upper bound on your performance: choosing the best every\ntime.  In ancient societies, nearly all work seems to have been of\nthis type.  The peasant had to decide whether a garment was worth\nmending, and the king whether or not to invade his neighbor, but\nneither was expected to invent anything.  In principle they could\nhave; the king could have invented firearms, then invaded his\nneighbor.  But in practice innovations were so rare that they weren't\nexpected of you, any more than goalkeepers are expected to score\ngoals. \n[9]\nIn practice, it seemed as if there was a correct decision\nin every situation, and if you made it you'd done your job perfectly,\njust as a goalkeeper who prevents the other team from scoring is\nconsidered to have played a perfect game.In this world, wisdom seemed paramount.  \n[10]\nEven now, most people\ndo work in which problems are put before them and they have to\nchoose the best alternative.  But as knowledge has grown more\nspecialized, there are more and more types of work in which people\nhave to make up new things, and in which performance is therefore\nunbounded.  Intelligence has become increasingly important relative\nto wisdom because there is more room for spikes.RecipesAnother sign we may have to choose between intelligence and wisdom\nis how different their recipes are.  Wisdom seems to come largely\nfrom curing childish qualities, and intelligence largely from\ncultivating them.Recipes for wisdom, particularly ancient ones, tend to have a\nremedial character.  To achieve wisdom one must cut away all the\ndebris that fills one's head on emergence from childhood, leaving\nonly the important stuff.  Both self-control and experience have\nthis effect: to eliminate the random biases that come from your own\nnature and from the circumstances of your upbringing respectively.\nThat's not all wisdom is, but it's a large part of it.  Much of\nwhat's in the sage's head is also in the head of every twelve year\nold.  The difference is that in the head of the twelve year old\nit's mixed together with a lot of random junk.The path to intelligence seems to be through working on hard problems.\nYou develop intelligence as you might develop muscles, through\nexercise.  But there can't be too much compulsion here.  No amount\nof discipline can replace genuine curiosity.  So cultivating\nintelligence seems to be a matter of identifying some bias in one's\ncharacter\u2014some tendency to be interested in certain types of\nthings\u2014and nurturing it.  Instead of obliterating your\nidiosyncrasies in an effort to make yourself a neutral vessel for\nthe truth, you select one and try to grow it from a seedling into\na tree.The wise are all much alike in their wisdom, but very smart people\ntend to be smart in distinctive ways.Most of our educational traditions aim at wisdom. So perhaps one\nreason schools work badly is that they're trying to make intelligence\nusing recipes for wisdom.  Most recipes for wisdom have an element\nof subjection.  At the very least, you're supposed to do what the\nteacher says.  The more extreme recipes aim to break down your\nindividuality the way basic training does.  But that's not the route\nto intelligence.  Whereas wisdom comes through humility, it may\nactually help, in cultivating intelligence, to have a mistakenly\nhigh opinion of your abilities, because that encourages you to keep\nworking.  Ideally till you realize how mistaken you were.(The reason it's hard to learn new skills late in life is not just\nthat one's brain is less malleable.  Another probably even worse\nobstacle is that one has higher standards.)I realize we're on dangerous ground here.  I'm not proposing the\nprimary goal of education should be to increase students' \"self-esteem.\"\nThat just breeds laziness.  And in any case, it doesn't really fool\nthe kids, not the smart ones.  They can tell at a young age that a\ncontest where everyone wins is a fraud.A teacher has to walk a narrow path: you want to encourage kids to\ncome up with things on their own, but you can't simply applaud\neverything they produce.  You have to be a good audience: appreciative,\nbut not too easily impressed.  And that's a lot of work.  You have\nto have a good enough grasp of kids' capacities at different ages\nto know when to be surprised.That's the opposite of traditional recipes for education.  Traditionally\nthe student is the audience, not the teacher; the student's job is\nnot to invent, but to absorb some prescribed body of material.  (The\nuse of the term \"recitation\" for sections in some colleges is a\nfossil of this.) The problem with these old traditions is that\nthey're too much influenced by recipes for wisdom.DifferentI deliberately gave this essay a provocative title; of course it's\nworth being wise.  But I think it's important to understand the\nrelationship between intelligence and wisdom, and particularly what\nseems to be the growing gap between them.  That way we can avoid\napplying rules and standards to intelligence that are really meant\nfor wisdom.  These two senses of \"knowing what to do\" are more\ndifferent than most people realize.  The path to wisdom is through\ndiscipline, and the path to intelligence through carefully selected\nself-indulgence.  Wisdom is universal, and intelligence idiosyncratic.\nAnd while wisdom yields calmness, intelligence much of the time\nleads to discontentment.That's particularly worth remembering.  A physicist friend recently\ntold me half his department was on Prozac.  Perhaps if we acknowledge\nthat some amount of frustration is inevitable in certain kinds\nof work, we can mitigate its effects.  Perhaps we can box it up and\nput it away some of the time, instead of letting it flow together\nwith everyday sadness to produce what seems an alarmingly large\npool.  At the very least, we can avoid being discontented about\nbeing discontented.If you feel exhausted, it's not necessarily because there's something\nwrong with you.  Maybe you're just running fast.Notes[1]\nGauss was supposedly asked this when he was 10.  Instead of\nlaboriously adding together the numbers like the other students,\nhe saw that they consisted of 50 pairs that each summed to 101 (100\n+ 1, 99 + 2, etc), and that he could just multiply 101 by 50 to get\nthe answer, 5050.[2]\nA variant is that intelligence is the ability to solve problems,\nand wisdom the judgement to know how to use those solutions.   But\nwhile this is certainly an important relationship between wisdom\nand intelligence, it's not the distinction between them.  Wisdom\nis useful in solving problems too, and intelligence can help in\ndeciding what to do with the solutions.[3]\nIn judging both intelligence and wisdom we have to factor out\nsome knowledge. People who know the combination of a safe will be\nbetter at opening it than people who don't, but no one would say\nthat was a test of intelligence or wisdom.But knowledge overlaps with wisdom and probably also intelligence.\nA knowledge of human nature is certainly part of wisdom.  So where\ndo we draw the line?Perhaps the solution is to discount knowledge that at some point\nhas a sharp drop in utility.  For example, understanding French\nwill help you in a large number of situations, but its value drops\nsharply as soon as no one else involved knows French.  Whereas the\nvalue of understanding vanity would decline more gradually.The knowledge whose utility drops sharply is the kind that has\nlittle relation to other knowledge.  This includes mere conventions,\nlike languages and safe combinations, and also what we'd call\n\"random\" facts, like movie stars' birthdays, or how to distinguish\n1956 from 1957 Studebakers.[4]\nPeople seeking some single thing called \"wisdom\" have been\nfooled by grammar.  Wisdom is just knowing the right thing to do,\nand there are a hundred and one different qualities that help in\nthat.  Some, like selflessness, might come from meditating in an\nempty room, and others, like a knowledge of human nature, might\ncome from going to drunken parties.Perhaps realizing this will help dispel the cloud of semi-sacred\nmystery that surrounds wisdom in so many people's eyes.  The mystery\ncomes mostly from looking for something that doesn't exist.  And\nthe reason there have historically been so many different schools\nof thought about how to achieve wisdom is that they've focused on\ndifferent components of it.When I use the word \"wisdom\" in this essay, I mean no more than\nwhatever collection of qualities helps people make the right choice\nin a wide variety of situations.[5]\nEven in English, our sense of the word \"intelligence\" is\nsurprisingly recent.  Predecessors like \"understanding\" seem to\nhave had a broader meaning.[6]\nThere is of course some uncertainty about how closely the remarks\nattributed to Confucius and Socrates resemble their actual opinions.\nI'm using these names as we use the name \"Homer,\" to mean the\nhypothetical people who said the things attributed to them.[7]\nAnalects VII:36, Fung trans.Some translators use \"calm\" instead of \"happy.\"  One source of\ndifficulty here is that present-day English speakers have a different\nidea of happiness from many older societies.  Every language probably\nhas a word meaning \"how one feels when things are going well,\" but\ndifferent cultures react differently when things go well.  We react\nlike children, with smiles and laughter.  But in a more reserved\nsociety, or in one where life was tougher, the reaction might be a\nquiet contentment.[8]\nIt may have been Andrew Wiles, but I'm not sure.  If anyone\nremembers such an interview, I'd appreciate hearing from you.[9]\nConfucius claimed proudly that he had never invented\nanything\u2014that he had simply passed on an accurate account of\nancient traditions.  [Analects VII:1] It's hard for us now to\nappreciate how important a duty it must have been in preliterate\nsocieties to remember and pass on the group's accumulated knowledge.\nEven in Confucius's time it still seems to have been the first duty\nof the scholar.[10]\nThe bias toward wisdom in ancient philosophy may be exaggerated\nby the fact that, in both Greece and China, many of the first\nphilosophers (including Confucius and Plato) saw themselves as\nteachers of administrators, and so thought disproportionately about\nsuch matters.  The few people who did invent things, like storytellers,\nmust have seemed an outlying data point that could be ignored.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston,\nand Robert Morris for reading drafts of this.November 2022Since I was about 9 I've been puzzled by the apparent contradiction\nbetween being made of matter that behaves in a predictable way, and\nthe feeling that I could choose to do whatever I wanted. At the\ntime I had a self-interested motive for exploring the question. At\nthat age (like most succeeding ages) I was always in trouble with\nthe authorities, and it seemed to me that there might possibly be\nsome way to get out of trouble by arguing that I wasn't responsible\nfor my actions. I gradually lost hope of that, but the puzzle\nremained: How do you reconcile being a machine made of matter with\nthe feeling that you're free to choose what you do?\n[1]The best way to explain the answer may be to start with a slightly\nwrong version, and then fix it. The wrong version is: You can do\nwhat you want, but you can't want what you want. Yes, you can control\nwhat you do, but you'll do what you want, and you can't control\nthat.The reason this is mistaken is that people do sometimes change what\nthey want. People who don't want to want something \u2014 drug addicts,\nfor example \u2014 can sometimes make themselves stop wanting it. And\npeople who want to want something \u2014 who want to like classical\nmusic, or broccoli \u2014 sometimes succeed.So we modify our initial statement: You can do what you want, but\nyou can't want to want what you want.That's still not quite true. It's possible to change what you want\nto want. I can imagine someone saying \"I decided to stop wanting\nto like classical music.\" But we're getting closer to the truth.\nIt's rare for people to change what they want to want, and the more\n\"want to\"s we add, the rarer it gets.We can get arbitrarily close to a true statement by adding more \"want\nto\"s in much the same way we can get arbitrarily close to 1 by adding\nmore 9s to a string of 9s following a decimal point. In practice\nthree or four \"want to\"s must surely be enough. It's hard even to\nenvision what it would mean to change what you want to want to want\nto want, let alone actually do it.So one way to express the correct answer is to use a regular\nexpression. You can do what you want, but there's some statement\nof the form \"you can't (want to)* want what you want\" that's true.\nUltimately you get back to a want that you don't control.\n[2]\nNotes[1]\nI didn't know when I was 9 that matter might behave randomly,\nbut I don't think it affects the problem much. Randomness destroys\nthe ghost in the machine as effectively as determinism.[2]\nIf you don't like using an expression, you can make the same\npoint using higher-order desires: There is some n such that you\ndon't control your nth-order desires.\nThanks to Trevor Blackwell,\nJessica Livingston, Robert Morris, and\nMichael Nielsen for reading drafts of this.May 2021Noora Health, a nonprofit I've \nsupported for years, just launched\na new NFT. It has a dramatic name, Save Thousands of Lives,\nbecause that's what the proceeds will do.Noora has been saving lives for 7 years. They run programs in\nhospitals in South Asia to teach new mothers how to take care of\ntheir babies once they get home. They're in 165 hospitals now. And\nbecause they know the numbers before and after they start at a new\nhospital, they can measure the impact they have. It is massive.\nFor every 1000 live births, they save 9 babies.This number comes from a study\nof 133,733 families at 28 different\nhospitals that Noora conducted in collaboration with the Better\nBirth team at Ariadne Labs, a joint center for health systems\ninnovation at Brigham and Women\u0092s Hospital and Harvard T.H. Chan\nSchool of Public Health.Noora is so effective that even if you measure their costs in the\nmost conservative way, by dividing their entire budget by the number\nof lives saved, the cost of saving a life is the lowest I've seen.\n$1,235.For this NFT, they're going to issue a public report tracking how\nthis specific tranche of money is spent, and estimating the number\nof lives saved as a result.NFTs are a new territory, and this way of using them is especially\nnew, but I'm excited about its potential. And I'm excited to see\nwhat happens with this particular auction, because unlike an NFT\nrepresenting something that has already happened,\nthis NFT gets better as the price gets higher.The reserve price was about $2.5 million, because that's what it\ntakes for the name to be accurate: that's what it costs to save\n2000 lives. But the higher the price of this NFT goes, the more\nlives will be saved. What a sentence to be able to write.December 2014I've read Villehardouin's chronicle of the Fourth Crusade at least\ntwo times, maybe three.  And yet if I had to write down everything\nI remember from it, I doubt it would amount to much more than a\npage.  Multiply this times several hundred, and I get an uneasy\nfeeling when I look at my bookshelves. What use is it to read all\nthese books if I remember so little from them?A few months ago, as I was reading Constance Reid's excellent\nbiography of Hilbert, I figured out if not the answer to this\nquestion, at least something that made me feel better about it.\nShe writes:\n\n  Hilbert had no patience with mathematical lectures which filled\n  the students with facts but did not teach them how to frame a\n  problem and solve it. He often used to tell them that \"a perfect\n  formulation of a problem is already half its solution.\"\n\nThat has always seemed to me an important point, and I was even\nmore convinced of it after hearing it confirmed by Hilbert.But how had I come to believe in this idea in the first place?  A\ncombination of my own experience and other things I'd read.  None\nof which I could at that moment remember!  And eventually I'd forget\nthat Hilbert had confirmed it too.  But my increased belief in the\nimportance of this idea would remain something I'd learned from\nthis book, even after I'd forgotten I'd learned it.Reading and experience train your model of the world.  And even if\nyou forget the experience or what you read, its effect on your model\nof the world persists.  Your mind is like a compiled program you've\nlost the source of.  It works, but you don't know why.The place to look for what I learned from Villehardouin's chronicle\nis not what I remember from it, but my mental models of the crusades,\nVenice, medieval culture, siege warfare, and so on.  Which doesn't\nmean I couldn't have read more attentively, but at least the harvest\nof reading is not so miserably small as it might seem.This is one of those things that seem obvious in retrospect.  But\nit was a surprise to me and presumably would be to anyone else who\nfelt uneasy about (apparently) forgetting so much they'd read.Realizing it does more than make you feel a little better about\nforgetting, though.  There are specific implications.For example, reading and experience are usually \"compiled\" at the\ntime they happen, using the state of your brain at that time.  The\nsame book would get compiled differently at different points in\nyour life.  Which means it is very much worth reading important\nbooks multiple times.  I always used to feel some misgivings about\nrereading books.  I unconsciously lumped reading together with work\nlike carpentry, where having to do something again is a sign you\ndid it wrong the first time.  Whereas now the phrase \"already read\"\nseems almost ill-formed.Intriguingly, this implication isn't limited to books.  Technology\nwill increasingly make it possible to relive our experiences.  When\npeople do that today it's usually to enjoy them again (e.g. when\nlooking at pictures of a trip) or to find the origin of some bug in\ntheir compiled code (e.g. when Stephen Fry succeeded in remembering\nthe childhood trauma that prevented him from singing).  But as\ntechnologies for recording and playing back your life improve, it\nmay become common for people to relive experiences without any goal\nin mind, simply to learn from them again as one might when rereading\na book.Eventually we may be able not just to play back experiences but\nalso to index and even edit them. So although not knowing how you\nknow things may seem part of being human, it may not be.\nThanks to Sam Altman, Jessica Livingston, and Robert Morris for reading \ndrafts of this.April 2004To the popular press, \"hacker\" means someone who breaks\ninto computers.  Among programmers it means a good programmer.\nBut the two meanings are connected.  To programmers,\n\"hacker\" connotes mastery in the most literal sense: someone\nwho can make a computer do what he wants\u2014whether the computer\nwants to or not.To add to the confusion, the noun \"hack\" also has two senses.  It can\nbe either a compliment or an insult.  It's called a hack when\nyou do something in an ugly way.  But when you do something\nso clever that you somehow beat the system, that's also\ncalled a hack.  The word is used more often in the former than\nthe latter sense, probably because ugly solutions are more\ncommon than brilliant ones.Believe it or not, the two senses of \"hack\" are also\nconnected.  Ugly and imaginative solutions have something in\ncommon: they both break the rules.  And there is a gradual\ncontinuum between rule breaking that's merely ugly (using\nduct tape to attach something to your bike) and rule breaking\nthat is brilliantly imaginative (discarding Euclidean space).Hacking predates computers.  When he\nwas working on the Manhattan Project, Richard Feynman used to\namuse himself by breaking into safes containing secret documents.\nThis tradition continues today.\nWhen we were in grad school, a hacker friend of mine who spent too much\ntime around MIT had\nhis own lock picking kit.\n(He now runs a hedge fund, a not unrelated enterprise.)It is sometimes hard to explain to authorities why one would\nwant to do such things.\nAnother friend of mine once got in trouble with the government for\nbreaking into computers.  This had only recently been declared\na crime, and the FBI found that their usual investigative\ntechnique didn't work.  Police investigation apparently begins with\na motive.  The usual motives are few: drugs, money, sex,\nrevenge.  Intellectual curiosity was not one of the motives on\nthe FBI's list.  Indeed, the whole concept seemed foreign to\nthem.Those in authority tend to be annoyed by hackers'\ngeneral attitude of disobedience.  But that disobedience is\na byproduct of the qualities that make them good programmers.\nThey may laugh at the CEO when he talks in generic corporate\nnewspeech, but they also laugh at someone who tells them\na certain problem can't be solved.\nSuppress one, and you suppress the other.This attitude is sometimes affected.  Sometimes young programmers\nnotice the eccentricities of eminent hackers and decide to\nadopt some of their own in order to seem smarter.\nThe fake version is not merely\nannoying; the prickly attitude of these posers\ncan actually slow the process of innovation.But even factoring in their annoying eccentricities,\nthe disobedient attitude of hackers is a net win.  I wish its\nadvantages were better understood.For example, I suspect people in Hollywood are\nsimply mystified by\nhackers' attitudes toward copyrights.  They are a perennial\ntopic of heated discussion on Slashdot.\nBut why should people who program computers\nbe so concerned about copyrights, of all things?Partly because some companies use mechanisms to prevent\ncopying.  Show any hacker a lock and his first thought is\nhow to pick it.  But there is a deeper reason that\nhackers are alarmed by measures like copyrights and patents.\nThey see increasingly aggressive measures to protect\n\"intellectual property\"\nas a threat to the intellectual\nfreedom they need to do their job.\nAnd they are right.It is by poking about inside current technology that\nhackers get ideas for the next generation.  No thanks,\nintellectual homeowners may say, we don't need any\noutside help.  But they're wrong.\nThe next generation of computer technology has\noften\u2014perhaps more often than not\u2014been developed by outsiders.In 1977 there was no doubt some group within IBM developing\nwhat they expected to be\nthe next generation of business computer.  They were mistaken.\nThe next generation of business computer was\nbeing developed on entirely different lines by two long-haired\nguys called Steve in a garage in Los Altos.  At about the\nsame time, the powers that be\nwere cooperating to develop the\nofficial next generation operating system, Multics.\nBut two guys who thought Multics excessively complex went off\nand wrote their own.  They gave it a name that\nwas a joking reference to Multics: Unix.The latest intellectual property laws impose\nunprecedented restrictions on the sort of poking around that\nleads to new ideas. In the past, a competitor might use patents\nto prevent you from selling a copy of something they\nmade, but they couldn't prevent you from\ntaking one apart to see how it worked.   The latest\nlaws make this a crime.  How are we\nto develop new technology if we can't study current\ntechnology to figure out how to improve it?Ironically, hackers have brought this on themselves.\nComputers are responsible for the problem.  The control systems\ninside machines used to be physical: gears and levers and cams.\nIncreasingly, the brains (and thus the value) of products is\nin software. And by this I mean software in the general sense:\ni.e. data.  A song on an LP is physically stamped into the\nplastic.  A song on an iPod's disk is merely stored on it.Data is by definition easy to copy.  And the Internet\nmakes copies easy to distribute.  So it is no wonder\ncompanies are afraid.  But, as so often happens, fear has\nclouded their judgement.  The government has responded\nwith draconian laws to protect intellectual property.\nThey probably mean well. But\nthey may not realize that such laws will do more harm\nthan good.Why are programmers so violently opposed to these laws?\nIf I were a legislator, I'd be interested in this\nmystery\u2014for the same reason that, if I were a farmer and suddenly\nheard a lot of squawking coming from my hen house one night,\nI'd want to go out and investigate.  Hackers are not stupid,\nand unanimity is very rare in this world.\nSo if they're all squawking,   \nperhaps there is something amiss.Could it be that such laws, though intended to protect America,\nwill actually harm it?  Think about it.  There is something\nvery American about Feynman breaking into safes during\nthe Manhattan Project.  It's hard to imagine the authorities\nhaving a sense of humor about such things over\nin Germany at that time.  Maybe it's not a coincidence.Hackers are unruly.  That is the essence of hacking.  And it\nis also the essence of Americanness.  It is no accident\nthat Silicon Valley\nis in America, and not France, or Germany,\nor England, or Japan. In those countries, people color inside\nthe lines.I lived for a while in Florence.  But after I'd been there\na few months I realized that what I'd been unconsciously hoping\nto find there was back in the place I'd just left.\nThe reason Florence is famous is that in 1450, it was New York.\nIn 1450 it was filled with the kind of turbulent and ambitious\npeople you find now in America.  (So I went back to America.)It is greatly to America's advantage that it is\na congenial atmosphere for the right sort of unruliness\u2014that\nit is a home not just for the smart, but for smart-alecks.\nAnd hackers are invariably smart-alecks.  If we had a national\nholiday, it would be April 1st.  It says a great deal about\nour work that we use the same word for a brilliant or a\nhorribly cheesy solution.   When we cook one up we're not\nalways 100% sure which kind it is.  But as long as it has\nthe right sort of wrongness, that's a promising sign.\nIt's odd that people\nthink of programming as precise and methodical.  Computers\nare precise and methodical.  Hacking is something you do\nwith a gleeful laugh.In our world some of the most characteristic solutions\nare not far removed from practical\njokes.  IBM was no doubt rather surprised by the consequences\nof the licensing deal for DOS, just as the hypothetical\n\"adversary\" must be when Michael Rabin solves a problem by\nredefining it as one that's easier to solve.Smart-alecks have to develop a keen sense of how much they\ncan get away with.  And lately hackers \nhave sensed a change\nin the atmosphere.\nLately hackerliness seems rather frowned upon.To hackers the recent contraction in civil liberties seems\nespecially ominous.  That must also mystify outsiders. \nWhy should we care especially about civil\nliberties?  Why programmers, more than\ndentists or salesmen or landscapers?Let me put the case in terms a government official would appreciate.\nCivil liberties are not just an ornament, or a quaint\nAmerican tradition.  Civil liberties make countries rich.\nIf you made a graph of\nGNP per capita vs. civil liberties, you'd notice a definite\ntrend.  Could civil liberties really be a cause, rather\nthan just an effect?  I think so.  I think a society in which\npeople can do and say what they want will also tend to\nbe one in which the most efficient solutions win, rather than\nthose sponsored by the most influential people.\nAuthoritarian countries become corrupt;\ncorrupt countries become poor; and poor countries are weak. \nIt seems to me there is\na Laffer curve for government power, just as for\ntax revenues.  At least, it seems likely enough that it\nwould be stupid to try the experiment and find out.  Unlike\nhigh tax rates, you can't repeal totalitarianism if it\nturns out to be a mistake.This is why hackers worry.  The government spying on people doesn't\nliterally make programmers write worse code.  It just leads\neventually to a world in which bad ideas win.  And because\nthis is so important to hackers, they're especially sensitive\nto it.  They can sense totalitarianism approaching from a\ndistance, as animals can sense an approaching  \nthunderstorm.It would be ironic if, as hackers fear, recent measures\nintended to protect national security and intellectual property\nturned out to be a missile aimed right at what makes   \nAmerica successful.  But it would not be the first time that\nmeasures taken in an atmosphere of panic had\nthe opposite of the intended effect.There is such a thing as Americanness.\nThere's nothing like living abroad to teach you that.   \nAnd if you want to know whether something will nurture or squash\nthis quality, it would be hard to find a better focus\ngroup than hackers, because they come closest of any group\nI know to embodying it.  Closer, probably,  than\nthe men running our government,\nwho for all their talk of patriotism\nremind me more of Richelieu or Mazarin\nthan Thomas Jefferson or George Washington.When you read what the founding fathers had to say for\nthemselves, they sound more like hackers.\n\"The spirit of resistance to government,\"\nJefferson wrote, \"is so valuable on certain occasions, that I wish\nit always to be kept alive.\"Imagine an American president saying that today.\nLike the remarks of an outspoken old grandmother, the sayings of\nthe founding fathers have embarrassed generations of\ntheir less confident successors.  They remind us where we come from.\nThey remind us that it is the people who break rules that are\nthe source of America's wealth and power.Those in a position to impose rules naturally want them to be\nobeyed.  But be careful what you ask for. You might get it.Thanks to Ken Anderson, Trevor Blackwell, Daniel Giffin, \nSarah Harlin,  Shiro Kawai, Jessica Livingston, Matz, \nJackie McDonough, Robert Morris, Eric Raymond, Guido van Rossum,\nDavid Weinberger, and\nSteven Wolfram for reading drafts of this essay.\n(The image shows Steves Jobs and Wozniak \nwith a \"blue box.\"\nPhoto by Margret Wozniak. Reproduced by permission of Steve\nWozniak.)July 2010What hard liquor, cigarettes, heroin, and crack have in common is\nthat they're all more concentrated forms of less addictive predecessors.\nMost if not all the things we describe as addictive are.  And the\nscary thing is, the process that created them is accelerating.We wouldn't want to stop it.  It's the same process that cures\ndiseases: technological progress.  Technological progress means\nmaking things do more of what we want.  When the thing we want is\nsomething we want to want, we consider technological progress good.\nIf some new technique makes solar cells x% more efficient, that\nseems strictly better.  When progress concentrates something we\ndon't want to want\u2014when it transforms opium into heroin\u2014it seems\nbad.  But it's the same process at work.\n[1]No one doubts this process is accelerating, which means increasing\nnumbers of things we like will be transformed into things we like\ntoo much.\n[2]As far as I know there's no word for something we like too much.\nThe closest is the colloquial sense of \"addictive.\" That usage has\nbecome increasingly common during my lifetime.  And it's clear why:\nthere are an increasing number of things we need it for.  At the\nextreme end of the spectrum are crack and meth.  Food has been\ntransformed by a combination of factory farming and innovations in\nfood processing into something with way more immediate bang for the\nbuck, and you can see the results in any town in America.  Checkers\nand solitaire have been replaced by World of Warcraft and FarmVille.\nTV has become much more engaging, and even so it can't compete with Facebook.The world is more addictive than it was 40 years ago.   And unless\nthe forms of technological progress that produced these things are\nsubject to different laws than technological progress in general,\nthe world will get more addictive in the next 40 years than it did\nin the last 40.The next 40 years will bring us some wonderful things.  I don't\nmean to imply they're all to be avoided.  Alcohol is a dangerous\ndrug, but I'd rather live in a world with wine than one without.\nMost people can coexist with alcohol; but you have to be careful.\nMore things we like will mean more things we have to be careful\nabout.Most people won't, unfortunately.  Which means that as the world\nbecomes more addictive, the two senses in which one can live a\nnormal life will be driven ever further apart.  One sense of \"normal\"\nis statistically normal: what everyone else does.  The other is the\nsense we mean when we talk about the normal operating range of a\npiece of machinery: what works best.These two senses are already quite far apart.  Already someone\ntrying to live well would seem eccentrically abstemious in most of\nthe US.  That phenomenon is only going to become more pronounced.\nYou can probably take it as a rule of thumb from now on that if\npeople don't think you're weird, you're living badly.Societies eventually develop antibodies to addictive new things.\nI've seen that happen with cigarettes.  When cigarettes first\nappeared, they spread the way an infectious disease spreads through\na previously isolated population.  Smoking rapidly became a\n(statistically) normal thing.  There were ashtrays everywhere.  We\nhad ashtrays in our house when I was a kid, even though neither of\nmy parents smoked.  You had to for guests.As knowledge spread about the dangers of smoking, customs changed.\nIn the last 20 years, smoking has been transformed from something\nthat seemed totally normal into a rather seedy habit: from something\nmovie stars did in publicity shots to something small huddles of\naddicts do outside the doors of office buildings.  A lot of the\nchange was due to legislation, of course, but the legislation\ncouldn't have happened if customs hadn't already changed.It took a while though\u2014on the order of 100 years.  And unless the\nrate at which social antibodies evolve can increase to match the\naccelerating rate at which technological progress throws off new\naddictions, we'll be increasingly unable to rely on customs to\nprotect us.\n[3]\nUnless we want to be canaries in the coal mine\nof each new addiction\u2014the people whose sad example becomes a\nlesson to future generations\u2014we'll have to figure out for ourselves\nwhat to avoid and how.  It will actually become a reasonable strategy\n(or a more reasonable strategy) to suspect \neverything new.In fact, even that won't be enough.  We'll have to worry not just\nabout new things, but also about existing things becoming more\naddictive.  That's what bit me.  I've avoided most addictions, but\nthe Internet got me because it became addictive while I was using\nit.\n[4]Most people I know have problems with Internet addiction.  We're\nall trying to figure out our own customs for getting free of it.\nThat's why I don't have an iPhone, for example; the last thing I\nwant is for the Internet to follow me out into the world.\n[5]\nMy latest trick is taking long hikes.  I used to think running was a\nbetter form of exercise than hiking because it took less time.  Now\nthe slowness of hiking seems an advantage, because the longer I\nspend on the trail, the longer I have to think without interruption.Sounds pretty eccentric, doesn't it?  It always will when you're\ntrying to solve problems where there are no customs yet to guide\nyou.  Maybe I can't plead Occam's razor; maybe I'm simply eccentric.\nBut if I'm right about the acceleration of addictiveness, then this\nkind of lonely squirming to avoid it will increasingly be the fate\nof anyone who wants to get things done.  We'll increasingly be\ndefined by what we say no to.\nNotes[1]\nCould you restrict technological progress to areas where you\nwanted it?  Only in a limited way, without becoming a police state.\nAnd even then your restrictions would have undesirable side effects.\n\"Good\" and \"bad\" technological progress aren't sharply differentiated,\nso you'd find you couldn't slow the latter without also slowing the\nformer.  And in any case, as Prohibition and the \"war on drugs\"\nshow, bans often do more harm than good.[2]\nTechnology has always been accelerating.  By Paleolithic\nstandards, technology evolved at a blistering pace in the Neolithic\nperiod.[3]\nUnless we mass produce social customs.  I suspect the recent\nresurgence of evangelical Christianity in the US is partly a reaction\nto drugs.  In desperation people reach for the sledgehammer; if\ntheir kids won't listen to them, maybe they'll listen to God.  But\nthat solution has broader consequences than just getting kids to\nsay no to drugs.  You end up saying no to \nscience as well.\nI worry we may be heading for a future in which only a few people\nplot their own itinerary through no-land, while everyone else books\na package tour.  Or worse still, has one booked for them by the\ngovernment.[4]\nPeople commonly use the word \"procrastination\" to describe\nwhat they do on the Internet.  It seems to me too mild to describe\nwhat's happening as merely not-doing-work.  We don't call it\nprocrastination when someone gets drunk instead of working.[5]\nSeveral people have told me they like the iPad because it\nlets them bring the Internet into situations where a laptop would\nbe too conspicuous.  In other words, it's a hip flask.  (This is\ntrue of the iPhone too, of course, but this advantage isn't as\nobvious because it reads as a phone, and everyone's used to those.)Thanks to Sam Altman, Patrick Collison, Jessica Livingston, and\nRobert Morris for reading drafts of this.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nMarch 2008, rev. June 2008Technology tends to separate normal from natural.  Our bodies\nweren't designed to eat the foods that people in rich countries eat, or\nto get so little exercise.  \nThere may be a similar problem with the way we work: \na normal job may be as bad for us intellectually as white flour\nor sugar is for us physically.I began to suspect this after spending several years working \nwith startup founders.  I've now worked with over 200 of them, and I've\nnoticed a definite difference between programmers working on their\nown startups and those working for large organizations.\nI wouldn't say founders seem happier, necessarily;\nstarting a startup can be very stressful. Maybe the best way to put\nit is to say that they're happier in the sense that your body is\nhappier during a long run than sitting on a sofa eating\ndoughnuts.Though they're statistically abnormal, startup founders seem to be\nworking in a way that's more natural for humans.I was in Africa last year and saw a lot of animals in the wild that\nI'd only seen in zoos before. It was remarkable how different they\nseemed. Particularly lions. Lions in the wild seem about ten times\nmore alive. They're like different animals. I suspect that working\nfor oneself feels better to humans in much the same way that living\nin the wild must feel better to a wide-ranging predator like a lion.\nLife in a zoo is easier, but it isn't the life they were designed\nfor.\nTreesWhat's so unnatural about working for a big company?  The root of\nthe problem is that humans weren't meant to work in such large\ngroups.Another thing you notice when you see animals in the wild is that\neach species thrives in groups of a certain size.  A herd of impalas\nmight have 100 adults; baboons maybe 20; lions rarely 10.  Humans\nalso seem designed to work in groups, and what I've read about\nhunter-gatherers accords with research on organizations and my own\nexperience to suggest roughly what the ideal size is: groups of 8\nwork well; by 20 they're getting hard to manage; and a group of 50\nis really unwieldy.\n[1]\nWhatever the upper limit is, we are clearly not meant to work in\ngroups of several hundred.  And yet\u2014for reasons having more\nto do with technology than human nature\u2014a great many people\nwork for companies with hundreds or thousands of employees.Companies know groups that large wouldn't work, so they divide\nthemselves into units small enough to work together.  But to\ncoordinate these they have to introduce something new: bosses.These smaller groups are always arranged in a tree structure.  Your\nboss is the point where your group attaches to the tree.  But when\nyou use this trick for dividing a large group into smaller ones,\nsomething strange happens that I've never heard anyone mention\nexplicitly.  In the group one level up from yours, your boss\nrepresents your entire group.  A group of 10 managers is not merely\na group of 10 people working together in the usual way.  It's really\na group of groups.  Which means for a group of 10 managers to work\ntogether as if they were simply a group of 10 individuals, the group\nworking for each manager would have to work as if they were a single\nperson\u2014the workers and manager would each share only one\nperson's worth of freedom between them.In practice a group of people are never able to act as if they were\none person.  But in a large organization divided into groups in\nthis way, the pressure is always in that direction.  Each group\ntries its best to work as if it were the small group of individuals\nthat humans were designed to work in.  That was the point of creating\nit.  And when you propagate that constraint, the result is that\neach person gets freedom of action in inverse proportion to the\nsize of the entire tree.\n[2]Anyone who's worked for a large organization has felt this.  You\ncan feel the difference between working for a company with 100\nemployees and one with 10,000, even if your group has only 10 people.\nCorn SyrupA group of 10 people within a large organization is a kind of fake\ntribe.  The number of people you interact with is about right.  But\nsomething is missing: individual initiative.  Tribes of hunter-gatherers\nhave much more freedom.  The leaders have a little more power than other\nmembers of the tribe, but they don't generally tell them what to\ndo and when the way a boss can.It's not your boss's fault.  The real problem is that in the group\nabove you in the hierarchy, your entire group is one virtual person.\nYour boss is just the way that constraint is imparted to you.So working in a group of 10 people within a large organization feels\nboth right and wrong at the same time.   On the surface it feels\nlike the kind of group you're meant to work in, but something major\nis missing.  A job at a big company is like high fructose corn\nsyrup: it has some of the qualities of things you're meant to like,\nbut is disastrously lacking in others.Indeed, food is an excellent metaphor to explain what's wrong with\nthe usual sort of job.For example, working for a big company is the default thing to do,\nat least for programmers.  How bad could it be?  Well, food shows\nthat pretty clearly.  If you were dropped at a random point in\nAmerica today, nearly all the food around you would be bad for you.\nHumans were not designed to eat white flour, refined sugar, high\nfructose corn syrup, and hydrogenated vegetable oil.  And yet if\nyou analyzed the contents of the average grocery store you'd probably\nfind these four ingredients accounted for most of the calories.\n\"Normal\" food is terribly bad for you.  The only people who eat\nwhat humans were actually designed to eat are a few Birkenstock-wearing\nweirdos in Berkeley.If \"normal\" food is so bad for us, why is it so common?  There are\ntwo main reasons. One is that it has more immediate appeal.  You\nmay feel lousy an hour after eating that pizza, but eating the first\ncouple bites feels great.  The other is economies of scale.\nProducing junk food scales; producing fresh vegetables doesn't.\nWhich means (a) junk food can be very cheap, and (b) it's worth\nspending a lot to market it.If people have to choose between something that's cheap, heavily\nmarketed, and appealing in the short term, and something that's\nexpensive, obscure, and appealing in the long term, which do you\nthink most will choose?It's the same with work.  The average MIT graduate wants to work\nat Google or Microsoft, because it's a recognized brand, it's safe,\nand they'll get paid a good salary right away.  It's the job\nequivalent of the pizza they had for lunch.  The drawbacks will\nonly become apparent later, and then only in a vague sense of\nmalaise.And founders and early employees of startups, meanwhile, are like\nthe Birkenstock-wearing weirdos of Berkeley:  though a tiny minority\nof the population, they're the ones living as humans are meant to.\nIn an artificial world, only extremists live naturally.\nProgrammersThe restrictiveness of big company jobs is particularly hard on\nprogrammers, because the essence of programming is to build new\nthings.  Sales people make much the same pitches every day; support\npeople answer much the same questions; but once you've written a\npiece of code you don't need to write it again.  So a programmer\nworking as programmers are meant to is always making new things.\nAnd when you're part of an organization whose structure gives each\nperson freedom in inverse proportion to the size of the tree, you're\ngoing to face resistance when you do something new.This seems an inevitable consequence of bigness.  It's true even\nin the smartest companies.  I was talking recently to a founder who\nconsidered starting a startup right out of college, but went to\nwork for Google instead because he thought he'd learn more there.\nHe didn't learn as much as he expected.  Programmers learn by doing,\nand most of the things he wanted to do, he couldn't\u2014sometimes\nbecause the company wouldn't let him, but often because the company's\ncode wouldn't let him.  Between the drag of legacy code, the overhead\nof doing development in such a large organization, and the restrictions\nimposed by interfaces owned by other groups, he could only try a\nfraction of the things he would have liked to.  He said he has\nlearned much more in his own startup, despite the fact that he has\nto do all the company's errands as well as programming, because at\nleast when he's programming he can do whatever he wants.An obstacle downstream propagates upstream.  If you're not allowed\nto implement new ideas, you stop having them.  And vice versa: when\nyou can do whatever you want, you have more ideas about what to do.\nSo working for yourself makes your brain more powerful in the same\nway a low-restriction exhaust system makes an engine more powerful.Working for yourself doesn't have to mean starting a startup, of\ncourse.  But a programmer deciding between a regular job at a big\ncompany and their own startup is probably going to learn more doing\nthe startup.You can adjust the amount of freedom you get by scaling the size\nof company you work for.  If you start the company, you'll have the\nmost freedom.  If you become one of the first 10 employees you'll\nhave almost as much freedom as the founders.  Even a company with\n100 people will feel different from one with 1000.Working for a small company doesn't ensure freedom.  The tree\nstructure of large organizations sets an upper bound on freedom,\nnot a lower bound.  The head of a small company may still choose\nto be a tyrant.  The point is that a large organization is compelled\nby its structure to be one.\nConsequencesThat has real consequences for both organizations and individuals.\nOne is that companies will inevitably slow down as they grow larger,\nno matter how hard they try to keep their startup mojo.  It's a\nconsequence of the tree structure that every large organization is\nforced to adopt.Or rather, a large organization could only avoid slowing down if\nthey avoided tree structure.  And since human nature limits the\nsize of group that can work together, the only way I can imagine\nfor larger groups to avoid tree structure would be to have no\nstructure: to have each group actually be independent, and to work\ntogether the way components of a market economy do.That might be worth exploring.  I suspect there are already some\nhighly partitionable businesses that lean this way.  But I don't\nknow any technology companies that have done it.There is one thing companies can do short of structuring themselves\nas sponges:  they can stay small.  If I'm right, then it really\npays to keep a company as small as it can be at every stage.\nParticularly a technology company.  Which means it's doubly important\nto hire the best people.  Mediocre hires hurt you twice: they get\nless done, but they also make you big, because you need more of\nthem to solve a given problem.For individuals the upshot is the same: aim small.  It will always\nsuck to work for large organizations, and the larger the organization,\nthe more it will suck.In an essay I wrote a couple years ago \nI advised graduating seniors\nto work for a couple years for another company before starting their\nown.  I'd modify that now.  Work for another company if you want\nto, but only for a small one, and if you want to start your own\nstartup, go ahead.The reason I suggested college graduates not start startups immediately\nwas that I felt most would fail.  And they will.  But ambitious\nprogrammers are better off doing their own thing and failing than\ngoing to work at a big company.  Certainly they'll learn more.  They\nmight even be better off financially.  A lot of people in their\nearly twenties get into debt, because their expenses grow even\nfaster than the salary that seemed so high when they left school.\nAt least if you start a startup and fail your net worth will be\nzero rather than negative.  \n[3]We've now funded so many different types of founders that we have\nenough data to see patterns, and there seems to be no benefit from\nworking for a big company.  The people who've worked for a few years\ndo seem better than the ones straight out of college, but only\nbecause they're that much older.The people who come to us from big companies often seem kind of\nconservative.  It's hard to say how much is because big companies\nmade them that way, and how much is the natural conservatism that\nmade them work for the big companies in the first place.  But\ncertainly a large part of it is learned.  I know because I've seen\nit burn off.Having seen that happen so many times is one of the things that\nconvinces me that working for oneself, or at least for a small\ngroup, is the natural way for programmers to live.  Founders arriving\nat Y Combinator often have the downtrodden air of refugees.  Three\nmonths later they're transformed: they have so much more \nconfidence\nthat they seem as if they've grown several inches taller. \n[4]\nStrange as this sounds, they seem both more worried and happier at the same\ntime.  Which is exactly how I'd describe the way lions seem in the\nwild.Watching employees get transformed into founders makes it clear\nthat the difference between the two is due mostly to environment\u2014and\nin particular that the environment in big companies is toxic to\nprogrammers.   In the first couple weeks of working on their own\nstartup they seem to come to life, because finally they're working\nthe way people are meant to.Notes[1]\nWhen I talk about humans being meant or designed to live a\ncertain way, I mean by evolution.[2]\nIt's not only the leaves who suffer.  The constraint propagates\nup as well as down.  So managers are constrained too; instead of\njust doing things, they have to act through subordinates.[3]\nDo not finance your startup with credit cards.  Financing a\nstartup with debt is usually a stupid move, and credit card debt\nstupidest of all.  Credit card debt is a bad idea, period.  It is\na trap set by evil companies for the desperate and the foolish.[4]\nThe founders we fund used to be younger (initially we encouraged\nundergrads to apply), and the first couple times I saw this I used\nto wonder if they were actually getting physically taller.Thanks to Trevor Blackwell, Ross Boucher, Aaron Iba, Abby\nKirigin, Ivan Kirigin, Jessica Livingston, and Robert Morris for\nreading drafts of this.\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nApril 2001, rev. April 2003(This article is derived from a talk given at the 2001 Franz\nDeveloper Symposium.)\nIn the summer of 1995, my friend Robert Morris and I\nstarted a startup called \nViaweb.  \nOur plan was to write\nsoftware that would let end users build online stores.\nWhat was novel about this software, at the time, was\nthat it ran on our server, using ordinary Web pages\nas the interface.A lot of people could have been having this idea at the\nsame time, of course, but as far as I know, Viaweb was\nthe first Web-based application.  It seemed such\na novel idea to us that we named the company after it:\nViaweb, because our software worked via the Web,\ninstead of running on your desktop computer.Another unusual thing about this software was that it\nwas written primarily in a programming language called\nLisp. It was one of the first big end-user\napplications to be written in Lisp, which up till then\nhad been used mostly in universities and research labs. [1]The Secret WeaponEric Raymond has written an essay called \"How to Become a Hacker,\"\nand in it, among other things, he tells would-be hackers what\nlanguages they should learn.  He suggests starting with Python and\nJava, because they are easy to learn.  The serious hacker will also\nwant to learn C, in order to hack Unix, and Perl for system\nadministration and cgi scripts.  Finally, the truly serious hacker\nshould consider learning Lisp:\n\n  Lisp is worth learning for the profound enlightenment experience\n  you will have when you finally get it; that experience will make\n  you a better programmer for the rest of your days, even if you\n  never actually use Lisp itself a lot.\n\nThis is the same argument you tend to hear for learning Latin.  It\nwon't get you a job, except perhaps as a classics professor, but\nit will improve your mind, and make you a better writer in languages\nyou do want to use, like English.But wait a minute.  This metaphor doesn't stretch that far.  The\nreason Latin won't get you a job is that no one speaks it.  If you\nwrite in Latin, no one can understand you.  But Lisp is a computer\nlanguage, and computers speak whatever language you, the programmer,\ntell them to.So if Lisp makes you a better programmer, like he says, why wouldn't\nyou want to use it? If a painter were offered a brush that would\nmake him a better painter, it seems to me that he would want to\nuse it in all his paintings, wouldn't he? I'm not trying to make\nfun of Eric Raymond here.  On the whole, his advice is good.  What\nhe says about Lisp is pretty much the conventional wisdom.  But\nthere is a contradiction in the conventional wisdom:  Lisp will\nmake you a better programmer, and yet you won't use it.Why not?  Programming languages are just tools, after all.  If Lisp\nreally does yield better programs, you should use it.  And if it\ndoesn't, then who needs it?This is not just a theoretical question.  Software is a very\ncompetitive business, prone to natural monopolies.  A company that\ngets software written faster and better will, all other things\nbeing equal, put its competitors out of business.  And when you're\nstarting a startup, you feel this very keenly.  Startups tend to\nbe an all or nothing proposition.  You either get rich, or you get\nnothing.  In a startup, if you bet on the wrong technology, your\ncompetitors will crush you.Robert and I both knew Lisp well, and we couldn't see any reason\nnot to trust our instincts and go with Lisp.  We knew that everyone\nelse was writing their software in C++ or Perl.  But we also knew\nthat that didn't mean anything.  If you chose technology that way,\nyou'd be running Windows.  When you choose technology, you have to\nignore what other people are doing, and consider only what will\nwork the best.This is especially true in a startup.  In a big company, you can\ndo what all the other big companies are doing.  But a startup can't\ndo what all the other startups do.  I don't think a lot of people\nrealize this, even in startups.The average big company grows at about ten percent a year.  So if\nyou're running a big company and you do everything the way the\naverage big company does it, you can expect to do as well as the\naverage big company-- that is, to grow about ten percent a year.The same thing will happen if you're running a startup, of course.\nIf you do everything the way the average startup does it, you should\nexpect average performance.  The problem here is, average performance\nmeans that you'll go out of business.  The survival rate for startups\nis way less than fifty percent.  So if you're running a startup,\nyou had better be doing something odd.  If not, you're in trouble.Back in 1995, we knew something that I don't think our competitors\nunderstood, and few understand even now:  when you're writing\nsoftware that only has to run on your own servers, you can use\nany language you want.  When you're writing desktop software,\nthere's a strong bias toward writing applications in the same\nlanguage as the operating system.  Ten years ago, writing applications\nmeant writing applications in C.  But with Web-based software,\nespecially when you have the source code of both the language and\nthe operating system, you can use whatever language you want.This new freedom is a double-edged sword, however.  Now that you\ncan use any language, you have to think about which one to use.\nCompanies that try to pretend nothing has changed risk finding that\ntheir competitors do not.If you can use any language, which do you use?  We chose Lisp.\nFor one thing, it was obvious that rapid development would be\nimportant in this market.  We were all starting from scratch, so\na company that could get new features done before its competitors\nwould have a big advantage.  We knew Lisp was a really good language\nfor writing software quickly, and server-based applications magnify\nthe effect of rapid development, because you can release software\nthe minute it's done.If other companies didn't want to use Lisp, so much the better.\nIt might give us a technological edge, and we needed all the help\nwe could get.  When we started Viaweb, we had no experience in\nbusiness.  We didn't know anything about marketing, or hiring\npeople, or raising money, or getting customers.  Neither of us had\never even had what you would call a real job.  The only thing we\nwere good at was writing software.  We hoped that would save us.\nAny advantage we could get in the software department, we would\ntake.So you could say that using Lisp was an experiment.  Our hypothesis\nwas that if we wrote our software in Lisp, we'd be able to get\nfeatures done faster than our competitors, and also to do things\nin our software that they couldn't do.  And because Lisp was so\nhigh-level, we wouldn't need a big development team, so our costs\nwould be lower.  If this were so, we could offer a better product\nfor less money, and still make a profit.  We would end up getting\nall the users, and our competitors would get none, and eventually\ngo out of business.  That was what we hoped would happen, anyway.What were the results of this experiment?  Somewhat surprisingly,\nit worked.  We eventually had many competitors, on the order of\ntwenty to thirty of them, but none of their software could compete\nwith ours.  We had a wysiwyg online store builder that ran on the\nserver and yet felt like a desktop application.  Our competitors\nhad cgi scripts.  And we were always far ahead of them in features.\nSometimes, in desperation, competitors would try to introduce\nfeatures that we didn't have.  But with Lisp our development cycle\nwas so fast that we could sometimes duplicate a new feature within\na day or two of a competitor announcing it in a press release.  By\nthe time journalists covering the press release got round to calling\nus, we would have the new feature too.It must have seemed to our competitors that we had some kind of\nsecret weapon-- that we were decoding their Enigma traffic or\nsomething.  In fact we did have a secret weapon, but it was simpler\nthan they realized.  No one was leaking news of their features to\nus.   We were just able to develop software faster than anyone\nthought possible.When I was about nine I happened to get hold of a copy of The Day\nof the Jackal, by Frederick Forsyth.  The main character is an\nassassin who is hired to kill the president of France.  The assassin\nhas to get past the police to get up to an apartment that overlooks\nthe president's route.  He walks right by them, dressed up as an\nold man on crutches, and they never suspect him.Our secret weapon was similar.  We wrote our software in a weird\nAI language, with a bizarre syntax full of parentheses.  For years\nit had annoyed me to hear Lisp described that way.  But now it\nworked to our advantage.  In business, there is nothing more valuable\nthan a technical advantage your competitors don't understand.  In\nbusiness, as in war, surprise is worth as much as force.And so, I'm a little embarrassed to say, I never said anything\npublicly about Lisp while we were working on Viaweb.  We never\nmentioned it to the press, and if you searched for Lisp on our Web\nsite, all you'd find were the titles of two books in my bio.  This\nwas no accident.  A startup should give its competitors as little\ninformation as possible.  If they didn't know what language our\nsoftware was written in, or didn't care, I wanted to keep it that\nway.[2]The people who understood our technology best were the customers.\nThey didn't care what language Viaweb was written in either, but\nthey noticed that it worked really well.  It let them build great\nlooking online stores literally in minutes.  And so, by word of\nmouth mostly, we got more and more users.  By the end of 1996 we\nhad about 70 stores online.  At the end of 1997 we had 500.  Six\nmonths later, when Yahoo bought us, we had 1070 users.  Today, as\nYahoo Store, this software continues to dominate its market.  It's\none of the more profitable pieces of Yahoo, and the stores built\nwith it are the foundation of Yahoo Shopping.  I left Yahoo in\n1999, so I don't know exactly how many users they have now, but\nthe last I heard there were about 20,000.\nThe Blub ParadoxWhat's so great about Lisp?  And if Lisp is so great, why doesn't\neveryone use it?  These sound like rhetorical questions, but actually\nthey have straightforward answers.  Lisp is so great not because\nof some magic quality visible only to devotees, but because it is\nsimply the most powerful language available.  And the reason everyone\ndoesn't use it is that programming languages are not merely\ntechnologies, but habits of mind as well, and nothing changes\nslower.  Of course, both these answers need explaining.I'll begin with a shockingly controversial statement:  programming\nlanguages vary in power.Few would dispute, at least, that high level languages are more\npowerful than machine language.  Most programmers today would agree\nthat you do not, ordinarily, want to program in machine language.\nInstead, you should program in a high-level language, and have a\ncompiler translate it into machine language for you.  This idea is\neven built into the hardware now: since the 1980s, instruction sets\nhave been designed for compilers rather than human programmers.Everyone knows it's a mistake to write your whole program by hand\nin machine language.  What's less often understood is that there\nis a more general principle here: that if you have a choice of\nseveral languages, it is, all other things being equal, a mistake\nto program in anything but the most powerful one. [3]There are many exceptions to this rule.  If you're writing a program\nthat has to work very closely with a program written in a certain\nlanguage, it might be a good idea to write the new program in the\nsame language.  If you're writing a program that only has to do\nsomething very simple, like number crunching or bit manipulation,\nyou may as well use a less abstract language, especially since it\nmay be slightly faster.  And if you're writing a short, throwaway\nprogram, you may be better off just using whatever language has\nthe best library functions for the task.  But in general, for\napplication software, you want to be using the most powerful\n(reasonably efficient) language you can get, and using anything\nelse is a mistake, of exactly the same kind, though possibly in a\nlesser degree, as programming in machine language.You can see that machine language is very low level.  But, at least\nas a kind of social convention, high-level languages are often all\ntreated as equivalent.  They're not.  Technically the term \"high-level\nlanguage\" doesn't mean anything very definite.  There's no dividing\nline with machine languages on one side and all the high-level\nlanguages on the other.  Languages fall along a continuum [4] of\nabstractness, from the most powerful all the way down to machine\nlanguages, which themselves vary in power.Consider Cobol.  Cobol is a high-level language, in the sense that\nit gets compiled into machine language.  Would anyone seriously\nargue that Cobol is equivalent in power to, say, Python?  It's\nprobably closer to machine language than Python.Or how about Perl 4?  Between Perl 4 and Perl 5, lexical closures\ngot added to the language.  Most Perl hackers would agree that Perl\n5 is more powerful than Perl 4.  But once you've admitted that,\nyou've admitted that one high level language can be more powerful\nthan another.  And it follows inexorably that, except in special\ncases, you ought to use the most powerful you can get.This idea is rarely followed to its conclusion, though.  After a\ncertain age, programmers rarely switch languages voluntarily.\nWhatever language people happen to be used to, they tend to consider\njust good enough.Programmers get very attached to their favorite languages, and I\ndon't want to hurt anyone's feelings, so to explain this point I'm\ngoing to use a hypothetical language called Blub.  Blub falls right\nin the middle of the abstractness continuum.  It is not the most\npowerful language, but it is more powerful than Cobol or machine\nlanguage.And in fact, our hypothetical Blub programmer wouldn't use either\nof them.  Of course he wouldn't program in machine language.  That's\nwhat compilers are for.  And as for Cobol, he doesn't know how\nanyone can get anything done with it.  It doesn't even have x (Blub\nfeature of your choice).As long as our hypothetical Blub programmer is looking down the\npower continuum, he knows he's looking down.  Languages less powerful\nthan Blub are obviously less powerful, because they're missing some\nfeature he's used to.  But when our hypothetical Blub programmer\nlooks in the other direction, up the power continuum, he doesn't\nrealize he's looking up.  What he sees are merely weird languages.\nHe probably considers them about equivalent in power to Blub, but\nwith all this other hairy stuff thrown in as well.  Blub is good\nenough for him, because he thinks in Blub.When we switch to the point of view of a programmer using any of\nthe languages higher up the power continuum, however, we find that\nhe in turn looks down upon Blub.  How can you get anything done in\nBlub? It doesn't even have y.By induction, the only programmers in a position to see all the\ndifferences in power between the various languages are those who\nunderstand the most powerful one.  (This is probably what Eric\nRaymond meant about Lisp making you a better programmer.) You can't\ntrust the opinions of the others, because of the Blub paradox:\nthey're satisfied with whatever language they happen to use, because\nit dictates the way they think about programs.I know this from my own experience, as a high school kid writing\nprograms in Basic.  That language didn't even support recursion.\nIt's hard to imagine writing programs without using recursion, but\nI didn't miss it at the time.  I thought in Basic.  And I was a\nwhiz at it.  Master of all I surveyed.The five languages that Eric Raymond recommends to hackers fall at\nvarious points on the power continuum.  Where they fall relative\nto one another is a sensitive topic.  What I will say is that I\nthink Lisp is at the top.  And to support this claim I'll tell you\nabout one of the things I find missing when I look at the other\nfour languages.  How can you get anything done in them, I think,\nwithout macros? [5]Many languages have something called a macro.  But Lisp macros are\nunique.  And believe it or not, what they do is related to the\nparentheses.  The designers of Lisp didn't put all those parentheses\nin the language just to be different.  To the Blub programmer, Lisp\ncode looks weird.  But those parentheses are there for a reason.\nThey are the outward evidence of a fundamental difference between\nLisp and other languages.Lisp code is made out of Lisp data objects.  And not in the trivial\nsense that the source files contain characters, and strings are\none of the data types supported by the language.  Lisp code, after\nit's read by the parser, is made of data structures that you can\ntraverse.If you understand how compilers work, what's really going on is\nnot so much that Lisp has a strange syntax as that Lisp has no\nsyntax.  You write programs in the parse trees that get generated\nwithin the compiler when other languages are parsed.  But these\nparse trees are fully accessible to your programs.  You can write\nprograms that manipulate them.  In Lisp, these programs are called\nmacros.  They are programs that write programs.Programs that write programs?  When would you ever want to do that?\nNot very often, if you think in Cobol.  All the time, if you think\nin Lisp.  It would be convenient here if I could give an example\nof a powerful macro, and say there! how about that?  But if I did,\nit would just look like gibberish to someone who didn't know Lisp;\nthere isn't room here to explain everything you'd need to know to\nunderstand what it meant.  In \nAnsi Common Lisp I tried to move\nthings along as fast as I could, and even so I didn't get to macros\nuntil page 160.But I think I can give a kind of argument that might be convincing.\nThe source code of the Viaweb editor was probably about 20-25%\nmacros.  Macros are harder to write than ordinary Lisp functions,\nand it's considered to be bad style to use them when they're not\nnecessary.  So every macro in that code is there because it has to\nbe.  What that means is that at least 20-25% of the code in this\nprogram is doing things that you can't easily do in any other\nlanguage.  However skeptical the Blub programmer might be about my\nclaims for the mysterious powers of Lisp, this ought to make him\ncurious.  We weren't writing this code for our own amusement.  We\nwere a tiny startup, programming as hard as we could in order to\nput technical barriers between us and our competitors.A suspicious person might begin to wonder if there was some\ncorrelation here.  A big chunk of our code was doing things that\nare very hard to do in other languages.  The resulting software\ndid things our competitors' software couldn't do.  Maybe there was\nsome kind of connection.  I encourage you to follow that thread.\nThere may be more to that old man hobbling along on his crutches\nthan meets the eye.Aikido for StartupsBut I don't expect to convince anyone \n(over 25) \nto go out and learn\nLisp.  The purpose of this article is not to change anyone's mind,\nbut to reassure people already interested in using Lisp-- people\nwho know that Lisp is a powerful language, but worry because it\nisn't widely used.  In a competitive situation, that's an advantage.\nLisp's power is multiplied by the fact that your competitors don't\nget it.If you think of using Lisp in a startup, you shouldn't worry that\nit isn't widely understood.  You should hope that it stays that\nway. And it's likely to.  It's the nature of programming languages\nto make most people satisfied with whatever they currently use.\nComputer hardware changes so much faster than personal habits that\nprogramming practice is usually ten to twenty years behind the\nprocessor.  At places like MIT they were writing programs in\nhigh-level languages in the early 1960s, but many companies continued\nto write code in machine language well into the 1980s.  I bet a\nlot of people continued to write machine language until the processor,\nlike a bartender eager to close up and go home, finally kicked them\nout by switching to a risc instruction set.Ordinarily technology changes fast.  But programming languages are\ndifferent: programming languages are not just technology, but what\nprogrammers think in.  They're half technology and half religion.[6]\nAnd so the median language, meaning whatever language the median\nprogrammer uses, moves as slow as an iceberg.  Garbage collection,\nintroduced by Lisp in about 1960, is now widely considered to be\na good thing.  Runtime typing, ditto, is growing in popularity.\nLexical closures, introduced by Lisp in the early 1970s, are now,\njust barely, on the radar screen.  Macros, introduced by Lisp in the\nmid 1960s, are still terra incognita.Obviously, the median language has enormous momentum.  I'm not\nproposing that you can fight this powerful force.  What I'm proposing\nis exactly the opposite: that, like a practitioner of Aikido, you\ncan use it against your opponents.If you work for a big company, this may not be easy.  You will have\na hard time convincing the pointy-haired boss to let you build\nthings in Lisp, when he has just read in the paper that some other\nlanguage is poised, like Ada was twenty years ago, to take over\nthe world.  But if you work for a startup that doesn't have\npointy-haired bosses yet, you can, like we did, turn the Blub\nparadox to your advantage:  you can use technology that your\ncompetitors, glued immovably to the median language, will never be\nable to match.If you ever do find yourself working for a startup, here's a handy\ntip for evaluating competitors.  Read their job listings.  Everything\nelse on their site may be stock photos or the prose equivalent,\nbut the job listings have to be specific about what they want, or\nthey'll get the wrong candidates.During the years we worked on Viaweb I read a lot of job descriptions.\nA new competitor seemed to emerge out of the woodwork every month\nor so.  The first thing I would do, after checking to see if they\nhad a live online demo, was look at their job listings.  After a\ncouple years of this I could tell which companies to worry about\nand which not to.  The more of an IT flavor the job descriptions\nhad, the less dangerous the company was.  The safest kind were the\nones that wanted Oracle experience.  You never had to worry about\nthose.  You were also safe if they said they wanted C++ or Java\ndevelopers.  If they wanted Perl or Python programmers, that would\nbe a bit frightening-- that's starting to sound like a company\nwhere the technical side, at least, is run by real hackers.  If I\nhad ever seen a job posting looking for Lisp hackers, I would have\nbeen really worried.\nNotes[1] Viaweb at first had two parts: the editor, written in Lisp,\nwhich people used to build their sites, and the ordering system,\nwritten in C, which handled orders.  The first version was mostly\nLisp, because the ordering system was small.  Later we added two\nmore modules, an image generator written in C, and a back-office\nmanager written mostly in Perl.In January 2003, Yahoo released a new version of the editor \nwritten in C++ and Perl.  It's hard to say whether the program is no\nlonger written in Lisp, though, because to translate this program\ninto C++ they literally had to write a Lisp interpreter: the source\nfiles of all the page-generating templates are still, as far as I\nknow,  Lisp code.  (See Greenspun's Tenth Rule.)[2] Robert Morris says that I didn't need to be secretive, because\neven if our competitors had known we were using Lisp, they wouldn't\nhave understood why:  \"If they were that smart they'd already be\nprogramming in Lisp.\"[3] All languages are equally powerful in the sense of being Turing\nequivalent, but that's not the sense of the word programmers care\nabout. (No one wants to program a Turing machine.)  The kind of\npower programmers care about may not be formally definable, but\none way to explain it would be to say that it refers to features\nyou could only get in the less powerful language by writing an\ninterpreter for the more powerful language in it. If language A\nhas an operator for removing spaces from strings and language B\ndoesn't, that probably doesn't make A more powerful, because you\ncan probably write a subroutine to do it in B.  But if A supports,\nsay, recursion, and B doesn't, that's not likely to be something\nyou can fix by writing library functions.[4] Note to nerds: or possibly a lattice, narrowing toward the top;\nit's not the shape that matters here but the idea that there is at\nleast a partial order.[5] It is a bit misleading to treat macros as a separate feature.\nIn practice their usefulness is greatly enhanced by other Lisp\nfeatures like lexical closures and rest parameters.[6] As a result, comparisons of programming languages either take\nthe form of religious wars or undergraduate textbooks so determinedly\nneutral that they're really works of anthropology.  People who\nvalue their peace, or want tenure, avoid the topic.  But the question\nis only half a religious one; there is something there worth\nstudying, especially if you want to design new languages.May 2001(This article was written as a kind of business plan for a\nnew language.\nSo it is missing (because it takes for granted) the most important\nfeature of a good programming language: very powerful abstractions.)A friend of mine once told an eminent operating systems\nexpert that he wanted to design a really good\nprogramming language.  The expert told him that it would be a\nwaste of time, that programming languages don't become popular\nor unpopular based on their merits, and so no matter how\ngood his language was, no one would use it.  At least, that\nwas what had happened to the language he had designed.What does make a language popular?  Do popular\nlanguages deserve their popularity?  Is it worth trying to\ndefine a good programming language?  How would you do it?I think the answers to these questions can be found by looking \nat hackers, and learning what they want.  Programming\nlanguages are for hackers, and a programming language\nis good as a programming language (rather than, say, an\nexercise in denotational semantics or compiler design)\nif and only if hackers like it.1 The Mechanics of PopularityIt's true, certainly, that most people don't choose programming\nlanguages simply based on their merits.  Most programmers are told\nwhat language to use by someone else.  And yet I think the effect\nof such external factors on the popularity of programming languages\nis not as great as it's sometimes thought to be. I think a bigger\nproblem is that a hacker's idea of a good programming language is\nnot the same as most language designers'.Between the two, the hacker's opinion is the one that matters.\nProgramming languages are not theorems. They're tools, designed\nfor people, and they have to be designed to suit human strengths\nand weaknesses as much as shoes have to be designed for human feet.\nIf a shoe pinches when you put it on, it's a bad shoe, however\nelegant it may be as a piece of sculpture.It may be that the majority of programmers can't tell a good language\nfrom a bad one. But that's no different with any other tool. It\ndoesn't mean that it's a waste of time to try designing a good\nlanguage. Expert hackers \ncan tell a good language when they see\none, and they'll use it. Expert hackers are a tiny minority,\nadmittedly, but that tiny minority write all the good software,\nand their influence is such that the rest of the programmers will\ntend to use whatever language they use. Often, indeed, it is not\nmerely influence but command: often the expert hackers are the very\npeople who, as their bosses or faculty advisors, tell the other\nprogrammers what language to use.The opinion of expert hackers is not the only force that determines\nthe relative popularity of programming languages \u2014 legacy software\n(Cobol) and hype (Ada, Java) also play a role \u2014 but I think it is\nthe most powerful force over the long term. Given an initial critical\nmass and enough time, a programming language probably becomes about\nas popular as it deserves to be. And popularity further separates\ngood languages from bad ones, because feedback from real live users\nalways leads to improvements. Look at how much any popular language\nhas changed during its life. Perl and Fortran are extreme cases,\nbut even Lisp has changed a lot. Lisp 1.5 didn't have macros, for\nexample; these evolved later, after hackers at MIT had spent a\ncouple years using Lisp to write real programs. [1]So whether or not a language has to be good to be popular, I think\na language has to be popular to be good. And it has to stay popular\nto stay good. The state of the art in programming languages doesn't\nstand still. And yet the Lisps we have today are still pretty much\nwhat they had at MIT in the mid-1980s, because that's the last time\nLisp had a sufficiently large and demanding user base.Of course, hackers have to know about a language before they can\nuse it. How are they to hear? From other hackers. But there has to\nbe some initial group of hackers using the language for others even\nto hear about it. I wonder how large this group has to be; how many\nusers make a critical mass? Off the top of my head, I'd say twenty.\nIf a language had twenty separate users, meaning twenty users who\ndecided on their own to use it, I'd consider it to be real.Getting there can't be easy. I would not be surprised if it is\nharder to get from zero to twenty than from twenty to a thousand.\nThe best way to get those initial twenty users is probably to use\na trojan horse: to give people an application they want, which\nhappens to be written in the new language.2 External FactorsLet's start by acknowledging one external factor that does affect\nthe popularity of a programming language. To become popular, a\nprogramming language has to be the scripting language of a popular\nsystem. Fortran and Cobol were the scripting languages of early\nIBM mainframes. C was the scripting language of Unix, and so, later,\nwas Perl. Tcl is the scripting language of Tk. Java and Javascript\nare intended to be the scripting languages of web browsers.Lisp is not a massively popular language because it is not the\nscripting language of a massively popular system. What popularity\nit retains dates back to the 1960s and 1970s, when it was the\nscripting language of MIT. A lot of the great programmers of the\nday were associated with MIT at some point. And in the early 1970s,\nbefore C, MIT's dialect of Lisp, called MacLisp, was one of the\nonly programming languages a serious hacker would want to use.Today Lisp is the scripting language of two moderately popular\nsystems, Emacs and Autocad, and for that reason I suspect that most\nof the Lisp programming done today is done in Emacs Lisp or AutoLisp.Programming languages don't exist in isolation. To hack is a\ntransitive verb \u2014 hackers are usually hacking something \u2014 and in\npractice languages are judged relative to whatever they're used to\nhack. So if you want to design a popular language, you either have\nto supply more than a language, or you have to design your language\nto replace the scripting language of some existing system.Common Lisp is unpopular partly because it's an orphan. It did\noriginally come with a system to hack: the Lisp Machine. But Lisp\nMachines (along with parallel computers) were steamrollered by the\nincreasing power of general purpose processors in the 1980s. Common\nLisp might have remained popular if it had been a good scripting\nlanguage for Unix. It is, alas, an atrociously bad one.One way to describe this situation is to say that a language isn't\njudged on its own merits. Another view is that a programming language\nreally isn't a programming language unless it's also the scripting\nlanguage of something. This only seems unfair if it comes as a\nsurprise. I think it's no more unfair than expecting a programming\nlanguage to have, say, an implementation. It's just part of what\na programming language is.A programming language does need a good implementation, of course,\nand this must be free. Companies will pay for software, but individual\nhackers won't, and it's the hackers you need to attract.A language also needs to have a book about it. The book should be\nthin, well-written, and full of good examples. K&R is the ideal\nhere. At the moment I'd almost say that a language has to have a\nbook published by O'Reilly. That's becoming the test of mattering\nto hackers.There should be online documentation as well. In fact, the book\ncan start as online documentation. But I don't think that physical\nbooks are outmoded yet. Their format is convenient, and the de\nfacto censorship imposed by publishers is a useful if imperfect\nfilter. Bookstores are one of the most important places for learning\nabout new languages.3 BrevityGiven that you can supply the three things any language needs \u2014 a\nfree implementation, a book, and something to hack \u2014 how do you\nmake a language that hackers will like?One thing hackers like is brevity. Hackers are lazy, in the same\nway that mathematicians and modernist architects are lazy: they\nhate anything extraneous. It would not be far from the truth to\nsay that a hacker about to write a program decides what language\nto use, at least subconsciously, based on the total number of\ncharacters he'll have to type. If this isn't precisely how hackers\nthink, a language designer would do well to act as if it were.It is a mistake to try to baby the user with long-winded expressions\nthat are meant to resemble English. Cobol is notorious for this\nflaw. A hacker would consider being asked to writeadd x to y giving zinstead ofz = x+yas something between an insult to his intelligence and a sin against\nGod.It has sometimes been said that Lisp should use first and rest\ninstead of car and cdr, because it would make programs easier to\nread. Maybe for the first couple hours. But a hacker can learn\nquickly enough that car means the first element of a list and cdr\nmeans the rest. Using first and rest means 50% more typing. And\nthey are also different lengths, meaning that the arguments won't\nline up when they're called, as car and cdr often are, in successive\nlines. I've found that it matters a lot how code lines up on the\npage. I can barely read Lisp code when it is set in a variable-width\nfont, and friends say this is true for other languages too.Brevity is one place where strongly typed languages lose. All other\nthings being equal, no one wants to begin a program with a bunch\nof declarations. Anything that can be implicit, should be.The individual tokens should be short as well. Perl and Common Lisp\noccupy opposite poles on this question. Perl programs can be almost\ncryptically dense, while the names of built-in Common Lisp operators\nare comically long. The designers of Common Lisp probably expected\nusers to have text editors that would type these long names for\nthem. But the cost of a long name is not just the cost of typing\nit. There is also the cost of reading it, and the cost of the space\nit takes up on your screen.4 HackabilityThere is one thing more important than brevity to a hacker: being\nable to do what you want. In the history of programming languages\na surprising amount of effort has gone into preventing programmers\nfrom doing things considered to be improper. This is a dangerously\npresumptuous plan. How can the language designer know what the\nprogrammer is going to need to do? I think language designers would\ndo better to consider their target user to be a genius who will\nneed to do things they never anticipated, rather than a bumbler\nwho needs to be protected from himself. The bumbler will shoot\nhimself in the foot anyway. You may save him from referring to\nvariables in another package, but you can't save him from writing\na badly designed program to solve the wrong problem, and taking\nforever to do it.Good programmers often want to do dangerous and unsavory things.\nBy unsavory I mean things that go behind whatever semantic facade\nthe language is trying to present: getting hold of the internal\nrepresentation of some high-level abstraction, for example. Hackers\nlike to hack, and hacking means getting inside things and second\nguessing the original designer.Let yourself be second guessed. When you make any tool, people use\nit in ways you didn't intend, and this is especially true of a\nhighly articulated tool like a programming language. Many a hacker\nwill want to tweak your semantic model in a way that you never\nimagined. I say, let them; give the programmer access to as much\ninternal stuff as you can without endangering runtime systems like\nthe garbage collector.In Common Lisp I have often wanted to iterate through the fields\nof a struct \u2014 to comb out references to a deleted object, for example,\nor find fields that are uninitialized. I know the structs are just\nvectors underneath. And yet I can't write a general purpose function\nthat I can call on any struct. I can only access the fields by\nname, because that's what a struct is supposed to mean.A hacker may only want to subvert the intended model of things once\nor twice in a big program. But what a difference it makes to be\nable to. And it may be more than a question of just solving a\nproblem. There is a kind of pleasure here too. Hackers share the\nsurgeon's secret pleasure in poking about in gross innards, the", "context_length": 47824, "depth_percent": 44}